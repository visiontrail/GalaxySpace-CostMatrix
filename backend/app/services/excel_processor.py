"""
Excel æ•°æ®å¤„ç†æœåŠ¡
è´Ÿè´£è¯»å–ã€åˆ†æã€å¤„ç† Excel æ–‡ä»¶
"""
import pandas as pd
import numpy as np
from openpyxl import load_workbook
from typing import Dict, List, Tuple, Any, Optional
from datetime import datetime
import re
import os
import time
from collections import Counter

from app.utils.logger import get_logger


class ExcelProcessor:
    """Excel å¤„ç†å™¨"""
    
    def __init__(self, file_path: str):
        self.file_path = file_path
        self.sheets_data: Dict[str, pd.DataFrame] = {}
        self.workbook = None
        self.logger = get_logger("excel_processor")
        self._attendance_cache: Optional[pd.DataFrame] = None
        self._travel_cache: Dict[str, pd.DataFrame] = {}
        self._combined_travel_cache: Optional[pd.DataFrame] = None
        
    def load_all_sheets(self, load_workbook_obj: bool = False) -> Dict[str, pd.DataFrame]:
        """
        åŠ è½½æ‰€æœ‰ Sheet æ•°æ®

        Args:
            load_workbook_obj: æ˜¯å¦åŒæ—¶åŠ è½½ openpyxl Workbook å¯¹è±¡ï¼ˆä»…åœ¨éœ€è¦å›å†™æ—¶å¯ç”¨ï¼‰
        """
        try:
            start = time.perf_counter()
            self.logger.info(f"å¼€å§‹è¯»å– Excel æ–‡ä»¶: {self.file_path}")
            all_sheets = pd.read_excel(self.file_path, sheet_name=None)
            elapsed = time.perf_counter() - start

            self.sheets_data = all_sheets
            self._attendance_cache = None
            self._travel_cache = {}
            self._combined_travel_cache = None

            sheet_names = ", ".join(all_sheets.keys())
            self.logger.info(f"Excel è¯»å–å®Œæˆï¼ˆ{sheet_names}ï¼‰ï¼Œè€—æ—¶ {elapsed:.2f}s")
            
            # è¾“å‡ºæ¯ä¸ª Sheet çš„åŸºæœ¬ä¿¡æ¯
            for sheet_name, df in all_sheets.items():
                self.logger.info(f"Sheet [{sheet_name}]: {len(df)} è¡Œ, {len(df.columns)} åˆ—")
                self.logger.info(f"  åˆ—å: {list(df.columns)}")
                if len(df) > 0:
                    self.logger.info(f"  å‰2è¡Œæ•°æ®é¢„è§ˆ:")
                    for idx in range(min(2, len(df))):
                        row_data = df.iloc[idx].to_dict()
                        self.logger.info(f"    è¡Œ{idx}: {row_data}")
            
            # éƒ¨åˆ†åˆ†æåœºæ™¯ä¸éœ€è¦ Workbookï¼Œä»…åœ¨å›å†™ç­‰åœºæ™¯æŒ‰éœ€åŠ è½½
            if load_workbook_obj:
                wb_start = time.perf_counter()
                self.workbook = load_workbook(self.file_path, keep_links=False)
                self.logger.info(f"openpyxl å·¥ä½œç°¿åŠ è½½å®Œæˆï¼Œè€—æ—¶ {time.perf_counter() - wb_start:.2f}s")
            else:
                self.workbook = None
            
            return self.sheets_data
        except Exception as e:
            raise Exception(f"è¯»å– Excel æ–‡ä»¶å¤±è´¥: {str(e)}")

    def get_sheet_names(self) -> List[str]:
        """
        ä»…è·å– Sheet åç§°ï¼Œé¿å…è¯»å–å…¨éƒ¨æ•°æ®å¯¼è‡´è€—æ—¶
        """
        try:
            workbook = load_workbook(
                self.file_path,
                read_only=True,
                data_only=True,
                keep_links=False
            )
            sheet_names = workbook.sheetnames
            workbook.close()
            return sheet_names
        except Exception as e:
            raise Exception(f"è¯»å– Excel Sheet åç§°å¤±è´¥: {str(e)}")
    
    def get_sheet(self, sheet_name: str) -> Optional[pd.DataFrame]:
        """è·å–æŒ‡å®š Sheet"""
        return self.sheets_data.get(sheet_name)
    
    def clean_attendance_data(self, use_cache: bool = True) -> pd.DataFrame:
        """
        æ¸…æ´—è€ƒå‹¤æ•°æ®ï¼ˆçŠ¶æ€æ˜ç»†ï¼‰
        """
        if use_cache and self._attendance_cache is not None:
            return self._attendance_cache

        df = self.get_sheet("çŠ¶æ€æ˜ç»†")
        if df is None:
            return pd.DataFrame()
        
        # æ ‡å‡†åŒ–åˆ—å
        df = df.copy()
        
        # å¤„ç†æ—¥æœŸæ ¼å¼
        if 'æ—¥æœŸ' in df.columns:
            df['æ—¥æœŸ'] = pd.to_datetime(df['æ—¥æœŸ'], errors='coerce')
        
        # å¤„ç†å·¥æ—¶æ•°æ®
        if 'å·¥æ—¶' in df.columns:
            df['å·¥æ—¶'] = pd.to_numeric(df['å·¥æ—¶'], errors='coerce')
        
        # åˆ é™¤ç©ºè¡Œ
        df = df.dropna(subset=['å§“å'], how='all')

        if use_cache:
            self._attendance_cache = df
        
        return df
    
    def clean_travel_data(self, sheet_name: str, use_cache: bool = True) -> pd.DataFrame:
        """
        æ¸…æ´—å·®æ—…æ•°æ®ï¼ˆæœºç¥¨/é…’åº—/ç«è½¦ç¥¨ï¼‰
        """
        if use_cache and sheet_name in self._travel_cache:
            return self._travel_cache[sheet_name]

        df = self.get_sheet(sheet_name)
        if df is None:
            self.logger.warning(f"[{sheet_name}] Sheet ä¸å­˜åœ¨")
            return pd.DataFrame()

        self.logger.info(f"[{sheet_name}] å¼€å§‹æ¸…æ´—æ•°æ® - åŸå§‹åˆ—å: {list(df.columns)}")
        self.logger.info(f"[{sheet_name}] åŸå§‹è¡Œæ•°: {len(df)}")

        df = df.copy()
        # æ ‡å‡†åŒ–åˆ—åï¼šå»é™¤é¦–å°¾ç©ºæ ¼ï¼Œé¿å…ä¸åŒæœˆä»½ Excel åˆ—åç»†å¾®å·®å¼‚å¯¼è‡´åŒ¹é…å¤±è´¥
        df.columns = [str(c).strip() for c in df.columns]
        original_df = df.copy()
        
        # å¤„ç†é‡‘é¢å­—æ®µ
        amount_col = 'æˆä¿¡é‡‘é¢' if 'æˆä¿¡é‡‘é¢' in df.columns else 'é‡‘é¢'
        self.logger.info(f"[{sheet_name}] é‡‘é¢åˆ—: {amount_col}")
        if amount_col in df.columns:
            df[amount_col] = pd.to_numeric(df[amount_col], errors='coerce')
            # å°† NaN å¡«å……ä¸º 0ï¼Œä½†ä¿ç•™æ‰€æœ‰è®°å½•
            df[amount_col] = df[amount_col].fillna(0)
            self.logger.info(f"[{sheet_name}] é‡‘é¢åˆ—æœ‰æ•ˆå€¼æ•°: {df[amount_col].notna().sum()}")
        else:
            self.logger.warning(f"[{sheet_name}] æœªæ‰¾åˆ°é‡‘é¢åˆ—ï¼ˆæˆä¿¡é‡‘é¢æˆ–é‡‘é¢ï¼‰")
        
        def _parse_datetime_avoiding_time_only(series: pd.Series) -> pd.Series:
            """
            å°†åˆ—è§£æä¸º datetimeï¼Œå¹¶é¿å…æŠŠçº¯æ—¶é—´å€¼ï¼ˆå¦‚ '22:17' æˆ– datetime.timeï¼‰è§£æä¸ºâ€œä»Šå¤©â€çš„æ—¥æœŸã€‚
            """
            if series is None:
                return pd.Series(dtype="datetime64[ns]")

            # å·²ç»æ˜¯ datetime ç±»å‹
            try:
                if pd.api.types.is_datetime64_any_dtype(series):
                    return pd.to_datetime(series, errors="coerce")
            except Exception:
                pass

            # å¤„ç† datetime.time æˆ–çº¯æ—¶é—´å­—ç¬¦ä¸²
            time_only_regex = re.compile(r"^\\s*\\d{1,2}:\\d{2}(:\\d{2})?\\s*$")

            def _is_time_obj(v: Any) -> bool:
                try:
                    from datetime import time as dt_time
                    return isinstance(v, dt_time)
                except Exception:
                    return False

            if series.dtype == object:
                cleaned = series.copy()
                mask_time_obj = cleaned.map(_is_time_obj)
                cleaned.loc[mask_time_obj] = None

                str_series = cleaned.astype(str)
                mask_time_str = str_series.str.match(time_only_regex, na=False)
                cleaned.loc[mask_time_str] = None

                return pd.to_datetime(cleaned, errors="coerce")

            return pd.to_datetime(series, errors="coerce")

        found_date_cols: List[str] = []
        if sheet_name == 'æœºç¥¨':
            if 'èµ·é£æ—¶é—´' in original_df.columns:
                df['èµ·é£æ—¥æœŸ'] = _parse_datetime_avoiding_time_only(original_df['èµ·é£æ—¶é—´'])
                found_date_cols.append('èµ·é£æ—¶é—´â†’èµ·é£æ—¥æœŸ')
            if 'èµ·é£æ—¶é—´.1' in original_df.columns:
                df['èµ·é£æ—¥æœŸ.1'] = _parse_datetime_avoiding_time_only(original_df['èµ·é£æ—¶é—´.1'])
                found_date_cols.append('èµ·é£æ—¶é—´.1â†’èµ·é£æ—¥æœŸ.1')
        elif sheet_name == 'é…’åº—':
            # ä»¥â€œå…¥ä½æ—¥æœŸâ€ä¸ºä¸»ï¼›è‹¥å­˜åœ¨â€œå…¥ä½æ—¶é—´â€ï¼ˆå«æ—¥æœŸæ—¶é—´ï¼‰ï¼Œè¡¥å……åˆ°â€œå…¥ä½æ—¥æœŸ.1â€
            if 'å…¥ä½æ—¥æœŸ' in original_df.columns:
                df['å…¥ä½æ—¥æœŸ'] = _parse_datetime_avoiding_time_only(original_df['å…¥ä½æ—¥æœŸ'])
                found_date_cols.append('å…¥ä½æ—¥æœŸ')
            if 'å…¥ä½æ—¶é—´' in original_df.columns:
                dt_full = _parse_datetime_avoiding_time_only(original_df['å…¥ä½æ—¶é—´'])
                if dt_full.notna().any():
                    df['å…¥ä½æ—¥æœŸ.1'] = dt_full
                    found_date_cols.append('å…¥ä½æ—¶é—´â†’å…¥ä½æ—¥æœŸ.1')
        elif sheet_name == 'ç«è½¦ç¥¨':
            # â€œå‡ºå‘æ—¥æœŸâ€æ˜¯å…³é”®æ—¥æœŸå­—æ®µã€‚æ—§é€»è¾‘ä¼šæŠŠâ€œå‡ºå‘æ—¶é—´â€(HH:MM)å†™å…¥â€œå‡ºå‘æ—¥æœŸâ€ï¼Œå¯¼è‡´æ—¥æœŸè¢«è§£ææˆâ€œä»Šå¤©â€ã€‚
            if 'å‡ºå‘æ—¥æœŸ' in original_df.columns:
                df['å‡ºå‘æ—¥æœŸ'] = _parse_datetime_avoiding_time_only(original_df['å‡ºå‘æ—¥æœŸ'])
                found_date_cols.append('å‡ºå‘æ—¥æœŸ')
            elif 'å‡ºå‘æ—¶é—´' in original_df.columns:
                # å…œåº•ï¼šæŸäº›æ¨¡æ¿å¯èƒ½åªæä¾›â€œå‡ºå‘æ—¶é—´â€(åŒ…å«æ—¥æœŸæ—¶é—´)
                df['å‡ºå‘æ—¥æœŸ'] = _parse_datetime_avoiding_time_only(original_df['å‡ºå‘æ—¶é—´'])
                found_date_cols.append('å‡ºå‘æ—¶é—´â†’å‡ºå‘æ—¥æœŸ')

            # å¦‚æœâ€œå‡ºå‘æ—¶é—´â€å­˜åœ¨ä¸”æ˜¯å®Œæ•´æ—¥æœŸæ—¶é—´ï¼Œå†™å…¥â€œå‡ºå‘æ—¥æœŸ.1â€ï¼›è‹¥ä»…æ˜¯æ—¶é—´å­—ç¬¦ä¸²ï¼Œåˆ™ä¸â€œå‡ºå‘æ—¥æœŸâ€ç»„åˆ
            if 'å‡ºå‘æ—¶é—´' in original_df.columns:
                dt_full = _parse_datetime_avoiding_time_only(original_df['å‡ºå‘æ—¶é—´'])
                if dt_full.notna().any():
                    df['å‡ºå‘æ—¥æœŸ.1'] = dt_full
                    found_date_cols.append('å‡ºå‘æ—¶é—´â†’å‡ºå‘æ—¥æœŸ.1')
                elif 'å‡ºå‘æ—¥æœŸ' in df.columns and df['å‡ºå‘æ—¥æœŸ'].notna().any():
                    time_str = original_df['å‡ºå‘æ—¶é—´'].astype(str).str.strip()
                    time_only_mask = time_str.str.match(r"^\\d{1,2}:\\d{2}(:\\d{2})?$", na=False)
                    if time_only_mask.any():
                        date_str = df['å‡ºå‘æ—¥æœŸ'].dt.strftime('%Y-%m-%d')
                        combined = pd.to_datetime(date_str + ' ' + time_str, errors='coerce')
                        df.loc[time_only_mask, 'å‡ºå‘æ—¥æœŸ.1'] = combined.loc[time_only_mask]
                        found_date_cols.append('å‡ºå‘æ—¥æœŸ+å‡ºå‘æ—¶é—´â†’å‡ºå‘æ—¥æœŸ.1')

        self.logger.info(f"[{sheet_name}] æ‰¾åˆ°çš„æ—¥æœŸåˆ—: {found_date_cols}")
        
        # ç»Ÿä¸€å·®æ—…äººå‘˜å§“åå­—æ®µ
        name_cols = [col for col in ['å·®æ—…äººå‘˜å§“å', 'é¢„è®¢äººå§“å'] if col in df.columns]
        self.logger.info(f"[{sheet_name}] æ‰¾åˆ°çš„å§“ååˆ—: {name_cols}")
        if 'å·®æ—…äººå‘˜å§“å' in df.columns:
            df['å§“å'] = df['å·®æ—…äººå‘˜å§“å']
        elif 'é¢„è®¢äººå§“å' in df.columns:
            df['å§“å'] = df['é¢„è®¢äººå§“å']
        else:
            self.logger.warning(f"[{sheet_name}] æœªæ‰¾åˆ°å§“ååˆ—ï¼ˆå·®æ—…äººå‘˜å§“åæˆ–é¢„è®¢äººå§“åï¼‰")

        self.logger.info(f"[{sheet_name}] æ¸…æ´—åè¡Œæ•°: {len(df)}")
        self.logger.info(f"[{sheet_name}] æœ€ç»ˆåˆ—å: {list(df.columns)}")

        if use_cache:
            self._travel_cache[sheet_name] = df
        
        return df

    def _get_combined_travel_df(self) -> pd.DataFrame:
        """
        è·å–å¸¦æ¶ˆè´¹æ—¥æœŸå’Œå·®æ—…ç±»å‹çš„æ±‡æ€»å·®æ—…æ•°æ®ï¼ˆä½¿ç”¨ç¼“å­˜é¿å…é‡å¤è®¡ç®—ï¼‰
        """
        if self._combined_travel_cache is not None:
            return self._combined_travel_cache

        frames: List[pd.DataFrame] = []
        date_columns = {
            'æœºç¥¨': ['èµ·é£æ—¥æœŸ', 'èµ·é£æ—¥æœŸ.1', 'èµ·é£æ—¶é—´', 'èµ·é£æ—¶é—´.1'],
            'é…’åº—': ['å…¥ä½æ—¥æœŸ', 'å…¥ä½æ—¶é—´'],
            'ç«è½¦ç¥¨': ['å‡ºå‘æ—¥æœŸ', 'å‡ºå‘æ—¶é—´']
        }

        for sheet_name, date_cols in date_columns.items():
            df = self.clean_travel_data(sheet_name)
            if df.empty:
                continue
            
            date_col = None
            for col in date_cols:
                if col in df.columns:
                    date_col = col
                    break
            
            if not date_col:
                continue

            temp = df[['å§“å', date_col]].copy()
            temp = temp[temp[date_col].notna()]
            if temp.empty:
                continue

            temp['æ¶ˆè´¹æ—¥æœŸ'] = temp[date_col].dt.date
            temp['å·®æ—…ç±»å‹'] = sheet_name
            frames.append(temp[['å§“å', 'æ¶ˆè´¹æ—¥æœŸ', 'å·®æ—…ç±»å‹']])

        combined = pd.concat(frames, ignore_index=True) if frames else pd.DataFrame(
            columns=['å§“å', 'æ¶ˆè´¹æ—¥æœŸ', 'å·®æ—…ç±»å‹']
        )
        self._combined_travel_cache = combined
        return combined
    
    def extract_project_code(self, project_str: str) -> Tuple[str, str]:
        """
        ä»é¡¹ç›®å­—æ®µæå–é¡¹ç›®ä»£ç å’Œåç§°
        æ ¼å¼: "05010013 å¸‚åœº-æ•´æ˜Ÿ..."
        """
        if pd.isna(project_str) or not isinstance(project_str, str):
            return "", ""
        
        # å°è¯•æå–é¡¹ç›®ä»£ç ï¼ˆé€šå¸¸æ˜¯å¼€å¤´çš„æ•°å­—ï¼‰
        match = re.match(r'(\d+)\s+(.*)', project_str.strip())
        if match:
            return match.group(1), match.group(2)
        
        return "", project_str
    
    def aggregate_project_costs(self, top_n: int = 20) -> List[Dict[str, Any]]:
        """
        é¡¹ç›®æˆæœ¬å½’é›†
        
        Args:
            top_n: è¿”å›å‰Nä¸ªé¡¹ç›®ï¼Œå…¶ä½™æ±‡æ€»åˆ°"å…¶ä»–"ï¼ˆé»˜è®¤20ï¼‰
        """
        self.logger.info("=" * 80)
        self.logger.info("å¼€å§‹æ‰§è¡Œé¡¹ç›®æˆæœ¬å½’é›† - è¯¦ç»†æ¨¡å¼ï¼ˆBackendæœåŠ¡ï¼‰")
        self.logger.info("=" * 80)
        
        results = []
        
        # å¤„ç†æ‰€æœ‰å·®æ—…ç›¸å…³çš„ Sheet
        travel_sheets = ['æœºç¥¨', 'é…’åº—', 'ç«è½¦ç¥¨']
        
        all_records = []
        sheet_stats = {}
        
        for sheet_name in travel_sheets:
            self.logger.info(f"\nğŸ“‹ å¤„ç†å·®æ—…è¡¨: {sheet_name}")
            
            # è·å–åŸå§‹æ•°æ®ï¼ˆæœªæ¸…æ´—ï¼‰ä»¥è·å–çœŸå®è¡Œæ•°
            df_raw = self.get_sheet(sheet_name)
            original_count = 0 if df_raw is None else len(df_raw)
            
            df = self.clean_travel_data(sheet_name)
            if df.empty:
                self.logger.warning(f"   âš ï¸  {sheet_name} æ•°æ®ä¸ºç©º")
                continue
            
            # æ£€æŸ¥æ˜¯å¦æœ‰é¡¹ç›®å­—æ®µ
            if 'é¡¹ç›®' not in df.columns:
                self.logger.warning(f"   âš ï¸  {sheet_name} ç¼ºå°‘'é¡¹ç›®'åˆ—")
                continue
            
            amount_col = 'æˆä¿¡é‡‘é¢' if 'æˆä¿¡é‡‘é¢' in df.columns else 'é‡‘é¢'
            date_cols = ['å‡ºå‘æ—¥æœŸ', 'å‡ºå‘æ—¥æœŸ.1', 'å‡ºå‘æ—¶é—´', 'èµ·é£æ—¥æœŸ', 'èµ·é£æ—¥æœŸ.1', 'èµ·é£æ—¶é—´', 'èµ·é£æ—¶é—´.1', 'å…¥ä½æ—¥æœŸ', 'å…¥ä½æ—¶é—´']
            date_col = next((col for col in date_cols if col in df.columns), None)
            
            self.logger.info(f"   - åŸå§‹è®°å½•æ•°: {original_count}")
            self.logger.info(f"   - æ¸…æ´—åè®°å½•æ•°: {len(df)}")
            self.logger.info(f"   - é‡‘é¢åˆ—: {amount_col}")
            
            # ç»Ÿè®¡ä¿¡æ¯
            record_count = 0
            empty_project_count = 0
            sheet_total_amount = 0
            
            for idx, row in df.iterrows():
                project_str = row.get('é¡¹ç›®', '')
                project_code, project_name = self.extract_project_code(project_str)
                amount = row.get(amount_col, 0)
                
                # ç©ºé¡¹ç›®ä½œä¸ºå•ç‹¬çš„é¡¹ç›®ç±»åˆ«å¤„ç†
                if not project_code:
                    project_code = 'ç©ºé¡¹ç›®'
                    project_name = 'æœªåˆ†é…é¡¹ç›®'
                    empty_project_count += 1
                
                all_records.append({
                    'project_code': project_code,
                    'project_name': project_name,
                    'amount': amount,
                    'type': sheet_name,
                    'person': row.get('å§“å', ''),
                    'date': row.get(date_col, '') if date_col else ''
                })
                record_count += 1
                sheet_total_amount += amount
                
                # è¾“å‡ºå‰3æ¡è®°å½•çš„è¯¦ç»†ä¿¡æ¯
                if record_count <= 3:
                    person = row.get('å§“å', 'æœªçŸ¥')
                    date_val = row.get(date_col, '') if date_col else ''
                    # å®‰å…¨çš„æ—¥æœŸæ ¼å¼åŒ–
                    if pd.notna(date_val) and hasattr(date_val, 'strftime'):
                        date_str = date_val.strftime('%Y-%m-%d')
                    else:
                        date_str = str(date_val) if pd.notna(date_val) else 'æœªçŸ¥'
                    self.logger.debug(f"      è®°å½•{record_count}: {project_code} | {person} | Â¥{amount:,.2f} | {date_str}")
            
            sheet_stats[sheet_name] = {
                'original_total': original_count,
                'cleaned_total': len(df),
                'record_count': record_count,
                'empty_project_count': empty_project_count,
                'amount': sheet_total_amount
            }
            
            # ç»Ÿè®¡é‡‘é¢åˆ†å¸ƒ - åŸºäºæ‰€æœ‰æ¸…æ´—åè®°å½•
            zero_amount_count = (df[amount_col] == 0).sum() if amount_col in df.columns else 0
            negative_amount_count = (df[amount_col] < 0).sum() if amount_col in df.columns else 0
            positive_amount_count = (df[amount_col] > 0).sum() if amount_col in df.columns else 0
            filtered_count = original_count - len(df)
            
            # è®¡ç®—æ­£æ•°/è´Ÿæ•°é‡‘é¢æ€»å’Œï¼ˆåŸºäºæ‰€æœ‰æ¸…æ´—åè®°å½•ï¼‰
            negative_amount_sum = df[df[amount_col] < 0][amount_col].sum() if amount_col in df.columns and negative_amount_count > 0 else 0
            positive_amount_sum = df[df[amount_col] > 0][amount_col].sum() if amount_col in df.columns and positive_amount_count > 0 else 0
            # æ‰€æœ‰è®°å½•çš„å‡€æ€»é‡‘é¢ï¼ˆç”¨äºæ—¥å¿—æ˜¾ç¤ºï¼Œç¡®ä¿ä¸æ­£æ•°+è´Ÿæ•°ä¸€è‡´ï¼‰
            sheet_all_amount = df[amount_col].sum() if amount_col in df.columns else 0
            
            self.logger.info(f"   âœ… å¤„ç†å®Œæˆ:")
            self.logger.info(f"      - æ€»è®°å½•æ•°: {record_count}")
            if empty_project_count > 0:
                self.logger.info(f"      - ç©ºé¡¹ç›®è®°å½•: {empty_project_count} (å·²å½’å…¥\"ç©ºé¡¹ç›®\"ç±»åˆ«)")
            if filtered_count > 0:
                self.logger.warning(f"      âš ï¸  æ•°æ®æ¸…æ´—æ—¶è¿‡æ»¤è®°å½•: {filtered_count}æ¡")
            
            # é‡‘é¢åˆ†å¸ƒç»Ÿè®¡ï¼ˆåŸºäºæ‰€æœ‰æ¸…æ´—åè®°å½•ï¼‰
            self.logger.info(f"      - é‡‘é¢åˆ†å¸ƒï¼ˆå…¨éƒ¨è®°å½•ï¼‰:")
            if positive_amount_count > 0:
                self.logger.info(f"         â€¢ æ­£æ•°é‡‘é¢: {positive_amount_count}æ¡, åˆè®¡ Â¥{positive_amount_sum:,.2f}")
            if negative_amount_count > 0:
                self.logger.warning(f"         â€¢ è´Ÿæ•°é‡‘é¢: {negative_amount_count}æ¡, åˆè®¡ Â¥{negative_amount_sum:,.2f} (é€€æ¬¾/è°ƒæ•´)")
            if zero_amount_count > 0:
                self.logger.info(f"         â€¢ é›¶å€¼é‡‘é¢: {zero_amount_count}æ¡")
            self.logger.info(f"         â€¢ å‡€æ€»é‡‘é¢: Â¥{sheet_all_amount:,.2f}")
        
        # è¾“å‡ºæ±‡æ€»ç»Ÿè®¡
        self.logger.info(f"\nğŸ“Š å·®æ—…æ•°æ®æ±‡æ€»:")
        original_total_records = sum(stats['original_total'] for stats in sheet_stats.values())
        cleaned_total_records = sum(stats['cleaned_total'] for stats in sheet_stats.values())
        total_records = sum(stats['record_count'] for stats in sheet_stats.values())
        empty_project_records = sum(stats['empty_project_count'] for stats in sheet_stats.values())
        total_amount = sum(stats['amount'] for stats in sheet_stats.values())
        
        self.logger.info(f"   - åŸå§‹æ€»è®°å½•æ•°: {original_total_records}")
        self.logger.info(f"   - æ¸…æ´—åæ€»è®°å½•æ•°: {cleaned_total_records}")
        self.logger.info(f"   - å¤„ç†è®°å½•æ•°: {total_records}")
        if empty_project_records > 0:
            self.logger.info(f"   - ç©ºé¡¹ç›®è®°å½•æ•°: {empty_project_records} (å·²å½’å…¥\"ç©ºé¡¹ç›®\"ç±»åˆ«)")
        if original_total_records > cleaned_total_records:
            filtered = original_total_records - cleaned_total_records
            self.logger.warning(f"   âš ï¸  æ•°æ®æ¸…æ´—è¿‡æ»¤äº† {filtered} æ¡è®°å½•ï¼ˆå¯èƒ½æ˜¯æ— æ•ˆæ•°æ®æˆ–åˆ é™¤è¡Œï¼‰")
        self.logger.info(f"   - å‡€æ€»é‡‘é¢: Â¥{total_amount:,.2f}")
        self.logger.info(f"   - ğŸ’¡ è¯´æ˜: è´Ÿæ•°é‡‘é¢ï¼ˆé€€æ¬¾/è°ƒæ•´ï¼‰å·²åŒ…å«åœ¨å‡€æ€»é‡‘é¢è®¡ç®—ä¸­")
        
        # æŒ‰é¡¹ç›®ä»£ç èšåˆ
        if all_records:
            self.logger.info(f"\nğŸ”„ å¼€å§‹èšåˆé¡¹ç›®æ•°æ®...")
            df_projects = pd.DataFrame(all_records)
            self.logger.debug(f"   - å¾…èšåˆè®°å½•æ•°: {len(df_projects)}")
            
            grouped = df_projects.groupby(['project_code', 'project_name']).agg({
                'amount': 'sum',
                'person': 'count'
            }).reset_index()
            
            # æŒ‰æˆæœ¬é™åºæ’åº
            grouped = grouped.sort_values('amount', ascending=False).reset_index(drop=True)
            
            total_count = len(grouped)
            self.logger.info(f"   âœ… èšåˆå®Œæˆï¼Œå…± {total_count} ä¸ªå”¯ä¸€é¡¹ç›®")
            
            # éªŒè¯é‡‘é¢æ€»å’Œ
            grouped_total = grouped['amount'].sum()
            if abs(grouped_total - total_amount) > 0.01:
                self.logger.error(f"   âš ï¸  é‡‘é¢éªŒè¯å¤±è´¥ï¼")
                self.logger.error(f"      åŸå§‹æ€»è®¡: Â¥{total_amount:,.2f}")
                self.logger.error(f"      èšåˆæ€»è®¡: Â¥{grouped_total:,.2f}")
            
            self.logger.info(f"\nğŸ† é¡¹ç›®æˆæœ¬æ’åï¼ˆTop {min(20, total_count)}ï¼‰:")

            # æ—¥å¿—å§‹ç»ˆåªæ˜¾ç¤ºå‰20ä¸ªé¡¹ç›®çš„è¯¦ç»†ä¿¡æ¯ï¼ˆä¿æŒæ—¥å¿—å¯è¯»æ€§ï¼‰
            log_top_n = min(20, total_count)

            # å¦‚æœé¡¹ç›®æ•°é‡è¶…è¿‡ top_nï¼Œå°†è¶…å‡ºéƒ¨åˆ†æ±‡æ€»åˆ°"å…¶ä»–"
            if total_count > top_n:
                self.logger.info(f"   - å±•ç¤ºå‰{top_n}ä¸ªé¡¹ç›®")
                self.logger.info(f"   - å…¶ä½™{total_count - top_n}ä¸ªé¡¹ç›®æ±‡æ€»åˆ°\"å…¶ä»–\"")

                # å‰ top_n ä¸ªé¡¹ç›®ï¼ˆæ·»åŠ åˆ°ç»“æœï¼‰
                for idx, row in grouped.head(top_n).iterrows():
                    project_details = df_projects[
                        df_projects['project_code'] == row['project_code']
                    ].to_dict('records')

                    # è®¡ç®—åˆ†ç±»æˆæœ¬
                    project_df = df_projects[df_projects['project_code'] == row['project_code']]
                    flight_cost = project_df[project_df['type'] == 'æœºç¥¨']['amount'].sum()
                    hotel_cost = project_df[project_df['type'] == 'é…’åº—']['amount'].sum()
                    train_cost = project_df[project_df['type'] == 'ç«è½¦ç¥¨']['amount'].sum()

                    # æ—¥å¿—åªè¾“å‡ºå‰20ä¸ª
                    if idx < log_top_n:
                        self.logger.info(f"\n   #{idx+1}. {row['project_code']} - {row['project_name']}")
                        self.logger.info(f"      æ€»æˆæœ¬: Â¥{row['amount']:,.2f} | è®¢å•æ•°: {int(row['person'])}")
                        self.logger.info(f"      â”œâ”€ æœºç¥¨: Â¥{flight_cost:,.2f}")
                        self.logger.info(f"      â”œâ”€ é…’åº—: Â¥{hotel_cost:,.2f}")
                        self.logger.info(f"      â””â”€ ç«è½¦ç¥¨: Â¥{train_cost:,.2f}")

                    results.append({
                        'project_code': row['project_code'],
                        'project_name': row['project_name'],
                        'total_cost': float(row['amount']),
                        'flight_cost': float(flight_cost),
                        'hotel_cost': float(hotel_cost),
                        'train_cost': float(train_cost),
                        'record_count': int(row['person']),
                        'details': project_details[:10]
                    })
                
                # æ±‡æ€»"å…¶ä»–"é¡¹ç›®
                others_df = grouped.iloc[top_n:]
                others_total_cost = float(others_df['amount'].sum())
                others_record_count = int(others_df['person'].sum())
                others_flight_cost = float(df_projects[df_projects['project_code'].isin(others_df['project_code']) & (df_projects['type'] == 'æœºç¥¨')]['amount'].sum())
                others_hotel_cost = float(df_projects[df_projects['project_code'].isin(others_df['project_code']) & (df_projects['type'] == 'é…’åº—')]['amount'].sum())
                others_train_cost = float(df_projects[df_projects['project_code'].isin(others_df['project_code']) & (df_projects['type'] == 'ç«è½¦ç¥¨')]['amount'].sum())
                
                self.logger.info(f"\n   #{top_n+1}. å…¶ä»–")
                self.logger.info(f"      æ±‡æ€»é¡¹ç›®æ•°: {total_count - top_n}")
                self.logger.info(f"      æ€»æˆæœ¬: Â¥{others_total_cost:,.2f} | è®¢å•æ•°: {others_record_count}")
                
                results.append({
                    'project_code': 'å…¶ä»–',
                    'project_name': f'å…¶ä»–é¡¹ç›®ï¼ˆ{total_count - top_n}ä¸ªï¼‰',
                    'total_cost': others_total_cost,
                    'flight_cost': others_flight_cost,
                    'hotel_cost': others_hotel_cost,
                    'train_cost': others_train_cost,
                    'record_count': others_record_count,
                    'details': []
                })
            else:
                # å¦‚æœä¸è¶…è¿‡ top_nï¼Œè¿”å›å…¨éƒ¨
                self.logger.info(f"   - é¡¹ç›®æ€»æ•°ä¸è¶…è¿‡{top_n}ï¼Œè¿”å›å…¨éƒ¨")
                
                for idx, row in grouped.iterrows():
                    project_details = df_projects[
                        df_projects['project_code'] == row['project_code']
                    ].to_dict('records')
                    
                    # è®¡ç®—åˆ†ç±»æˆæœ¬
                    project_df = df_projects[df_projects['project_code'] == row['project_code']]
                    flight_cost = project_df[project_df['type'] == 'æœºç¥¨']['amount'].sum()
                    hotel_cost = project_df[project_df['type'] == 'é…’åº—']['amount'].sum()
                    train_cost = project_df[project_df['type'] == 'ç«è½¦ç¥¨']['amount'].sum()
                    
                    self.logger.info(f"\n   #{idx+1}. {row['project_code']} - {row['project_name']}")
                    self.logger.info(f"      æ€»æˆæœ¬: Â¥{row['amount']:,.2f} | è®¢å•æ•°: {int(row['person'])}")
                    self.logger.info(f"      â”œâ”€ æœºç¥¨: Â¥{flight_cost:,.2f}")
                    self.logger.info(f"      â”œâ”€ é…’åº—: Â¥{hotel_cost:,.2f}")
                    self.logger.info(f"      â””â”€ ç«è½¦ç¥¨: Â¥{train_cost:,.2f}")
                    
                    results.append({
                        'project_code': row['project_code'],
                        'project_name': row['project_name'],
                        'total_cost': float(row['amount']),
                        'flight_cost': float(flight_cost),
                        'hotel_cost': float(hotel_cost),
                        'train_cost': float(train_cost),
                        'record_count': int(row['person']),
                        'details': project_details[:10]
                    })
            
            # æœ€ç»ˆæ±‡æ€»
            self.logger.info(f"\n" + "=" * 80)
            self.logger.info(f"âœ… é¡¹ç›®æˆæœ¬å½’é›†å®Œæˆ")
            self.logger.info(f"=" * 80)
            self.logger.info(f"ğŸ“Š æœ€ç»ˆç»Ÿè®¡:")
            self.logger.info(f"   - è¿”å›é¡¹ç›®æ•°: {len(results)}")
            self.logger.info(f"   - æ€»æˆæœ¬: Â¥{sum(r['total_cost'] for r in results):,.2f}")
            self.logger.info(f"   - æ€»è®¢å•æ•°: {sum(r['record_count'] for r in results)}")
            self.logger.info("=" * 80 + "\n")
        else:
            self.logger.warning("âš ï¸  æ²¡æœ‰æ‰¾åˆ°ä»»ä½•é¡¹ç›®è®°å½•")

        return results, total_count
    
    def cross_check_attendance_travel(self) -> List[Dict[str, Any]]:
        """
        äº¤å‰éªŒè¯ï¼šè€ƒå‹¤æ•°æ® vs å·®æ—…æ•°æ®

        å¼‚å¸¸å®šä¹‰ï¼šè€ƒå‹¤çŠ¶æ€ç²¾ç¡®ä¸º"ä¸Šç­"ï¼ˆåœ¨åŠå…¬å®¤å·¥ä½œï¼‰ï¼Œä½†åŒä¸€å¤©æœ‰å·®æ—…æ¶ˆè´¹ï¼ˆå‡ºå·®åœ¨å¤–ï¼‰
        - "ä¸Šç­" + æœ‰å·®æ—…æ¶ˆè´¹ = å¼‚å¸¸ï¼ˆæ—¶é—´å’Œåœ°ç‚¹å†²çªï¼‰
        - "å…¬ä¼‘æ—¥ä¸Šç­" + æœ‰å·®æ—…æ¶ˆè´¹ = æ­£å¸¸ï¼ˆå‘¨æœ«åŠ ç­å‡ºå·®ï¼‰
        - "å‡ºå·®" + æœ‰å·®æ—…æ¶ˆè´¹ = æ­£å¸¸ï¼ˆå‡ºå·®çŠ¶æ€ï¼‰
        """
        anomalies = []

        # è·å–è€ƒå‹¤æ•°æ®
        attendance_df = self.clean_attendance_data()
        if attendance_df.empty or 'å½“æ—¥çŠ¶æ€åˆ¤æ–­' not in attendance_df.columns:
            return anomalies
        if 'æ—¥æœŸ' not in attendance_df.columns:
            return anomalies

        # ä»…ä¿ç•™æœ‰æ—¥æœŸçš„æ•°æ®ï¼Œæå‰è®¡ç®—æ—¥æœŸå­—æ®µï¼Œé¿å…åç»­é‡å¤è½¬æ¢
        attendance_df = attendance_df.dropna(subset=['æ—¥æœŸ']).copy()
        if attendance_df.empty:
            return anomalies

        attendance_df['æ—¥æœŸ'] = attendance_df['æ—¥æœŸ'].dt.date
        attendance_df['å½“æ—¥çŠ¶æ€åˆ¤æ–­'] = attendance_df['å½“æ—¥çŠ¶æ€åˆ¤æ–­'].astype(str)
        if 'ä¸€çº§éƒ¨é—¨' in attendance_df.columns:
            attendance_df['ä¸€çº§éƒ¨é—¨'] = attendance_df['ä¸€çº§éƒ¨é—¨'].fillna('æœªçŸ¥éƒ¨é—¨')
        else:
            attendance_df['ä¸€çº§éƒ¨é—¨'] = 'æœªçŸ¥éƒ¨é—¨'

        # åªå…³æ³¨è€ƒå‹¤çŠ¶æ€ç²¾ç¡®ä¸º"ä¸Šç­"çš„è®°å½•ï¼ˆæ’é™¤"å…¬ä¼‘æ—¥ä¸Šç­"ã€"å‡ºå·®"ç­‰ï¼‰
        # çœŸæ­£çš„å¼‚å¸¸æ˜¯ï¼šåœ¨åŠå…¬å®¤ä¸Šç­ï¼Œä½†åŒä¸€å¤©æœ‰å·®æ—…æ¶ˆè´¹
        work_attendance = attendance_df[
            attendance_df['å½“æ—¥çŠ¶æ€åˆ¤æ–­'] == 'ä¸Šç­'
        ]
        if work_attendance.empty:
            return anomalies

        # èšåˆæ‰€æœ‰å·®æ—…æ•°æ®ï¼ˆå§“å + æ¶ˆè´¹æ—¥æœŸ + å·®æ—…ç±»å‹ï¼‰ï¼Œå¹¶ç¼“å­˜
        travel_df = self._get_combined_travel_df()
        if travel_df.empty:
            return anomalies

        travel_grouped = (
            travel_df.groupby(['å§“å', 'æ¶ˆè´¹æ—¥æœŸ'])['å·®æ—…ç±»å‹']
            .apply(list)
            .reset_index()
            .rename(columns={'æ¶ˆè´¹æ—¥æœŸ': 'æ—¥æœŸ'})
        )

        # åŸºäºå§“å+æ—¥æœŸä¸€æ¬¡æ€§å…³è”ï¼Œæ‰¾å‡ºä¸Šç­ä½†æœ‰å·®æ—…æ¶ˆè´¹çš„è®°å½•
        merged = work_attendance.merge(
            travel_grouped,
            on=['å§“å', 'æ—¥æœŸ'],
            how='inner'
        )

        for _, row in merged.iterrows():
            date_val = row.get('æ—¥æœŸ')
            date_str = date_val.strftime('%Y-%m-%d') if hasattr(date_val, 'strftime') else str(date_val)
            travel_list = row.get('å·®æ—…ç±»å‹', []) or []
            name = row.get('å§“å', '')
            anomalies.append({
                'name': name,
                'date': date_str,
                'department': row.get('ä¸€çº§éƒ¨é—¨', 'æœªçŸ¥éƒ¨é—¨'),
                'anomaly_type': 'A',
                'attendance_status': row.get('å½“æ—¥çŠ¶æ€åˆ¤æ–­', ''),
                'travel_records': travel_list,
                'description': f'{name} åœ¨ {date_str} è€ƒå‹¤æ˜¾ç¤ºä¸Šç­ï¼ˆåœ¨åŠå…¬å®¤ï¼‰ï¼Œä½†æœ‰ {",".join(travel_list)} æ¶ˆè´¹è®°å½•ï¼ˆå‡ºå·®åœ¨å¤–ï¼‰ï¼Œå­˜åœ¨æ—¶é—´å’Œåœ°ç‚¹å†²çª'
            })

        self.logger.info(f"äº¤å‰éªŒè¯å®Œæˆï¼Œå‘ç° {len(anomalies)} æ¡å¼‚å¸¸è®°å½•ï¼ˆä¸Šç­çŠ¶æ€æœ‰å·®æ—…æ¶ˆè´¹ï¼‰")
        return anomalies
    
    def analyze_booking_behavior(self) -> Dict[str, Any]:
        """
        é¢„è®¢è¡Œä¸ºåˆ†æï¼ˆæœºç¥¨ï¼‰
        """
        df = self.clean_travel_data('æœºç¥¨')
        if df.empty or 'æå‰é¢„å®šå¤©æ•°' not in df.columns:
            return {}
        
        # è¿‡æ»¤æœ‰æ•ˆæ•°æ®
        valid_df = df[df['æå‰é¢„å®šå¤©æ•°'].notna() & (df['æå‰é¢„å®šå¤©æ•°'] >= 0)]
        
        if valid_df.empty:
            return {}
        
        amount_col = 'æˆä¿¡é‡‘é¢' if 'æˆä¿¡é‡‘é¢' in valid_df.columns else 'é‡‘é¢'
        
        # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
        avg_advance = float(valid_df['æå‰é¢„å®šå¤©æ•°'].mean())
        
        # ç›¸å…³æ€§åˆ†æ
        correlation = float(valid_df[['æå‰é¢„å®šå¤©æ•°', amount_col]].corr().iloc[0, 1])
        
        # æå‰å¤©æ•°åˆ†å¸ƒ
        advance_distribution = valid_df['æå‰é¢„å®šå¤©æ•°'].value_counts().to_dict()
        advance_distribution = {str(int(k)): int(v) for k, v in advance_distribution.items()}
        
        # æŒ‰æå‰å¤©æ•°åˆ†ç»„çš„å¹³å‡æˆæœ¬
        cost_by_advance = valid_df.groupby('æå‰é¢„å®šå¤©æ•°')[amount_col].mean().reset_index()
        cost_by_advance_list = [
            {'advance_days': int(row['æå‰é¢„å®šå¤©æ•°']), 'avg_cost': float(row[amount_col])}
            for _, row in cost_by_advance.iterrows()
        ]
        
        return {
            'avg_advance_days': round(avg_advance, 2),
            'correlation_advance_cost': round(correlation, 3),
            'advance_day_distribution': advance_distribution,
            'cost_by_advance_days': sorted(cost_by_advance_list, key=lambda x: x['advance_days'])
        }

    def count_over_standard_orders(self) -> Dict[str, Any]:
        """
        ç»Ÿè®¡å„å·®æ—…ç±»å‹çš„è¶…æ ‡è®¢å•æ•°é‡

        ä¸šåŠ¡è§„åˆ™ï¼š
        - æœºç¥¨ï¼šè¶…æ ‡ç±»å‹åŒ…å«â€œè¶…æŠ˜æ‰£â€æˆ–â€œè¶…æ—¶é—´â€
        - é…’åº—ï¼šæŒ‰â€œæ˜¯å¦è¶…æ ‡â€ä¸ºâ€œæ˜¯â€
        - ç«è½¦ç¥¨ï¼šæŒ‰â€œæ˜¯å¦è¶…æ ‡â€ä¸ºâ€œæ˜¯â€
        """
        flight_df = self.clean_travel_data('æœºç¥¨')
        hotel_df = self.clean_travel_data('é…’åº—')
        train_df = self.clean_travel_data('ç«è½¦ç¥¨')

        def _count_yes(df: pd.DataFrame, column: str) -> int:
            if df.empty or column not in df.columns:
                return 0
            return int(df[df[column].astype(str).str.contains('æ˜¯', na=False)].shape[0])

        def _extract_over_types(value: str) -> List[str]:
            """
            æå–æœºç¥¨çš„è¶…æ ‡ç±»å‹æ ‡ç­¾
            - æ”¯æŒä»¥ç©ºæ ¼ã€é€—å·ã€åˆ†å·ã€æ–œæ ç­‰åˆ†éš”çš„å¤šæ ‡ç­¾æ ¼å¼
            - è‹¥å­—ç¬¦ä¸²ä¸­åŒ…å«å·²çŸ¥å…³é”®å­—ï¼ˆè¶…æŠ˜æ‰£/è¶…æ—¶é—´ï¼‰ä½†æœªåˆ†éš”ï¼Œä¹Ÿèƒ½æ•è·
            """
            if not value or pd.isna(value):
                return []

            raw = str(value)
            tokens = []

            # å¸¸è§åˆ†éš”ç¬¦æ‹†åˆ†
            for part in re.split(r'[;,ï¼Œã€/\\s]+', raw):
                cleaned = part.strip()
                if cleaned and 'è¶…' in cleaned:
                    tokens.append(cleaned)

            # å…œåº•ï¼šå¤„ç†æœªæ˜¾å¼åˆ†éš”ä½†åŒ…å«å…³é”®å­—çš„åœºæ™¯
            for keyword in ['è¶…æŠ˜æ‰£', 'è¶…æ—¶é—´']:
                if keyword in raw and keyword not in tokens:
                    tokens.append(keyword)

            return tokens

        flight_over = 0
        flight_over_type_counter: Counter[str] = Counter()
        if not flight_df.empty:
            if 'è¶…æ ‡ç±»å‹' in flight_df.columns:
                # ç»Ÿè®¡æ•°é‡
                flight_over = int(
                    flight_df[
                        flight_df['è¶…æ ‡ç±»å‹']
                        .astype(str)
                        .str.contains('è¶…æŠ˜æ‰£|è¶…æ—¶é—´', na=False)
                    ].shape[0]
                )

                # ç»Ÿè®¡ç±»å‹åˆ†å¸ƒ
                type_series = flight_df['è¶…æ ‡ç±»å‹'].dropna().astype(str)
                for raw_type in type_series:
                    for token in _extract_over_types(raw_type):
                        flight_over_type_counter[token] += 1

                # å¦‚æœæœ‰ç±»å‹åˆ†å¸ƒä½†æœªåŒ¹é…åˆ°æ•°é‡ï¼Œç”¨åˆ†å¸ƒæ±‚å’Œå…œåº•
                if flight_over == 0 and flight_over_type_counter:
                    flight_over = sum(flight_over_type_counter.values())
            elif 'æ˜¯å¦è¶…æ ‡' in flight_df.columns:
                flight_over = _count_yes(flight_df, 'æ˜¯å¦è¶…æ ‡')
                if flight_over > 0:
                    flight_over_type_counter['æœªæ³¨æ˜ç±»å‹'] += flight_over

        hotel_over = _count_yes(hotel_df, 'æ˜¯å¦è¶…æ ‡')
        train_over = _count_yes(train_df, 'æ˜¯å¦è¶…æ ‡')

        total = flight_over + hotel_over + train_over

        return {
            'total': int(total),
            'flight': int(flight_over),
            'hotel': int(hotel_over),
            'train': int(train_over),
            'flight_over_types': {k: int(v) for k, v in flight_over_type_counter.items()},
        }

    def count_total_orders(self) -> Dict[str, int]:
        """
        ç»Ÿè®¡å„å·®æ—…ç±»å‹åŠæ€»è®¢å•æ•°
        """
        flight_df = self.clean_travel_data('æœºç¥¨')
        hotel_df = self.clean_travel_data('é…’åº—')
        train_df = self.clean_travel_data('ç«è½¦ç¥¨')

        flight = 0 if flight_df is None else int(len(flight_df))
        hotel = 0 if hotel_df is None else int(len(hotel_df))
        train = 0 if train_df is None else int(len(train_df))

        return {
            'total': int(flight + hotel + train),
            'flight': flight,
            'hotel': hotel,
            'train': train,
        }
    
    def calculate_department_costs(self, top_n: int = 15) -> List[Dict[str, Any]]:
        """
        éƒ¨é—¨æˆæœ¬æ±‡æ€»ï¼ˆåŒ…å«å¹³å‡å·¥æ—¶å’Œäººæ•°ç»Ÿè®¡ï¼‰
        
        Args:
            top_n: è¿”å›å‰Nä¸ªéƒ¨é—¨ï¼Œå…¶ä½™æ±‡æ€»åˆ°"å…¶ä»–"ï¼ˆé»˜è®¤15ï¼‰
        """
        results = []
        
        # è·å–è€ƒå‹¤æ•°æ®ä»¥è®¡ç®—å·¥æ—¶å’Œäººæ•°
        attendance_df = self.clean_attendance_data()
        dept_attendance_stats = {}
        
        if not attendance_df.empty and 'ä¸€çº§éƒ¨é—¨' in attendance_df.columns:
            # è®¡ç®—æ¯ä¸ªéƒ¨é—¨çš„å¹³å‡å·¥æ—¶å’Œäººæ•°
            for dept in attendance_df['ä¸€çº§éƒ¨é—¨'].unique():
                if pd.isna(dept):
                    continue
                dept_data = attendance_df[attendance_df['ä¸€çº§éƒ¨é—¨'] == dept]
                avg_hours = 0
                if 'å·¥æ—¶' in dept_data.columns:
                    valid_hours = dept_data[dept_data['å·¥æ—¶'] != 0]['å·¥æ—¶'].dropna()
                    if not valid_hours.empty:
                        avg_hours = float(valid_hours.mean())
                    if pd.isna(avg_hours):
                        avg_hours = 0
                person_count = dept_data['å§“å'].nunique() if 'å§“å' in dept_data.columns else 0
                dept_attendance_stats[dept] = {
                    'avg_hours': float(avg_hours),
                    'person_count': int(person_count)
                }
        
        # å§‹ç»ˆä»æ˜ç»†è¡¨è®¡ç®—éƒ¨é—¨æˆæœ¬ï¼ˆä¸ä½¿ç”¨"å·®æ—…æ±‡æ€»" sheetï¼‰
        travel_data = {
            'æœºç¥¨': 'flight_cost',
            'é…’åº—': 'hotel_cost',
            'ç«è½¦ç¥¨': 'train_cost'
        }
        
        dept_costs = {}
        
        for sheet_name, cost_key in travel_data.items():
            df = self.clean_travel_data(sheet_name)
            if df.empty:
                continue
            
            # å°è¯•å…³è”éƒ¨é—¨ä¿¡æ¯ï¼ˆä¼˜å…ˆä½¿ç”¨å·®æ—…è¡¨ä¸­çš„éƒ¨é—¨ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä»è€ƒå‹¤è¡¨è·å–ï¼‰
            df = df.copy()
            if 'ä¸€çº§éƒ¨é—¨' in df.columns:
                # å·®æ—…è¡¨å·²æœ‰éƒ¨é—¨ä¿¡æ¯ï¼Œä¼˜å…ˆä½¿ç”¨
                pass
            elif not attendance_df.empty and 'å§“å' in attendance_df.columns and 'ä¸€çº§éƒ¨é—¨' in attendance_df.columns:
                # ä»è€ƒå‹¤è¡¨è·å–éƒ¨é—¨ä¿¡æ¯
                name_dept = attendance_df[['å§“å', 'ä¸€çº§éƒ¨é—¨']].drop_duplicates()
                df = df.merge(name_dept, on='å§“å', how='left')

            if 'ä¸€çº§éƒ¨é—¨' not in df.columns:
                continue

            amount_col = 'æˆä¿¡é‡‘é¢' if 'æˆä¿¡é‡‘é¢' in df.columns else 'é‡‘é¢'

            for _, row in df.iterrows():
                dept = row.get('ä¸€çº§éƒ¨é—¨')
                # å¤„ç†éƒ¨é—¨ä¸ºç©ºçš„æƒ…å†µ
                if pd.isna(dept) or (isinstance(dept, str) and dept.strip() == ''):
                    dept = 'æœªçŸ¥éƒ¨é—¨'
                else:
                    dept = str(dept).strip()
                
                if dept not in dept_costs:
                    stats = dept_attendance_stats.get(dept, {'avg_hours': 0, 'person_count': 0})
                    dept_costs[dept] = {
                        'department': dept,
                        'total_cost': 0,
                        'flight_cost': 0,
                        'hotel_cost': 0,
                        'train_cost': 0,
                        'avg_hours': stats['avg_hours'],
                        'person_count': stats['person_count']
                    }
                
                amount = row.get(amount_col, 0) or 0
                dept_costs[dept][cost_key] += amount
                dept_costs[dept]['total_cost'] += amount
        
        results = list(dept_costs.values())
        results = sorted(results, key=lambda x: x['total_cost'], reverse=True)
        
        # åº”ç”¨ top_n é™åˆ¶å¹¶æ·»åŠ "å…¶ä»–"
        return self._apply_top_n_with_others(results, top_n, 'department')
    
    def _apply_top_n_with_others(self, results: List[Dict[str, Any]], top_n: int, name_key: str) -> List[Dict[str, Any]]:
        """
        åº”ç”¨ Top N é™åˆ¶ï¼Œå°†è¶…å‡ºéƒ¨åˆ†æ±‡æ€»åˆ°"å…¶ä»–"
        
        Args:
            results: å·²æ’åºçš„ç»“æœåˆ—è¡¨
            top_n: ä¿ç•™çš„å‰Næ¡è®°å½•
            name_key: åç§°å­—æ®µï¼ˆ'department' æˆ– 'project_code'ï¼‰
        
        Returns:
            å¤„ç†åçš„ç»“æœåˆ—è¡¨
        """
        if not results or len(results) <= top_n:
            return results
        
        # å‰ top_n æ¡
        top_results = results[:top_n]
        
        # å‰©ä½™çš„æ±‡æ€»åˆ°"å…¶ä»–"
        others = results[top_n:]
        total_count = len(results)
        
        others_summary = {
            name_key: 'å…¶ä»–',
            'total_cost': sum(item.get('total_cost', 0) for item in others),
            'flight_cost': sum(item.get('flight_cost', 0) for item in others),
            'hotel_cost': sum(item.get('hotel_cost', 0) for item in others),
            'train_cost': sum(item.get('train_cost', 0) for item in others),
        }
        
        # å¦‚æœæ˜¯éƒ¨é—¨æ•°æ®ï¼Œè®¡ç®—å¹³å‡å·¥æ—¶å’Œæ€»äººæ•°
        if name_key == 'department':
            avg_hours_list = [item.get('avg_hours', 0) for item in others if item.get('avg_hours', 0) > 0]
            others_summary['avg_hours'] = sum(avg_hours_list) / len(avg_hours_list) if avg_hours_list else 0
            others_summary['person_count'] = sum(item.get('person_count', 0) for item in others)
        
        # å¦‚æœæ˜¯é¡¹ç›®æ•°æ®
        if name_key == 'project_code':
            others_summary['project_name'] = f'å…¶ä»–é¡¹ç›®ï¼ˆ{total_count - top_n}ä¸ªï¼‰'
            others_summary['record_count'] = sum(item.get('record_count', 0) for item in others)
            others_summary['details'] = []
        
        top_results.append(others_summary)
        
        return top_results
    
    def get_attendance_summary(self) -> Dict[str, Any]:
        """
        è€ƒå‹¤æ•°æ®æ±‡æ€»
        """
        df = self.clean_attendance_data()
        if df.empty:
            return {}
        
        total_records = len(df)
        total_persons = df['å§“å'].nunique() if 'å§“å' in df.columns else 0
        
        status_distribution = {}
        if 'å½“æ—¥çŠ¶æ€åˆ¤æ–­' in df.columns:
            status_distribution = df['å½“æ—¥çŠ¶æ€åˆ¤æ–­'].value_counts().to_dict()
        
        avg_work_hours = 0
        if 'å·¥æ—¶' in df.columns:
            valid_hours = df[df['å·¥æ—¶'] != 0]['å·¥æ—¶'].dropna()
            if not valid_hours.empty:
                avg_work_hours = float(valid_hours.mean())
        
        return {
            'total_records': total_records,
            'total_persons': total_persons,
            'status_distribution': status_distribution,
            'avg_work_hours': round(avg_work_hours, 2)
        }
    
    def write_analysis_results(self, results: Dict[str, Any], output_path: Optional[str] = None) -> str:
        """
        å°†åˆ†æç»“æœå›å†™åˆ° Excelï¼ˆæ–°å¢ Sheetï¼‰
        ä½¿ç”¨ openpyxl ä¿ç•™åŸæ ¼å¼
        """
        if output_path is None:
            base_name = os.path.splitext(self.file_path)[0]
            output_path = f"{base_name}_analyzed.xlsx"
        
        if self.workbook is None:
            self.workbook = load_workbook(self.file_path)
        
        # åˆ›å»ºåˆ†æç»“æœ Sheet
        sheet_name = "åˆ†æç»“æœ"
        if sheet_name in self.workbook.sheetnames:
            del self.workbook[sheet_name]
        
        ws = self.workbook.create_sheet(sheet_name)
        
        # å†™å…¥æ ‡é¢˜
        ws.append(["CostMatrix æ•°æ®åˆ†ææŠ¥å‘Š"])
        ws.append([f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"])
        ws.append([])
        
        # å†™å…¥é¡¹ç›®æˆæœ¬
        if 'project_costs' in results and results['project_costs']:
            ws.append(["é¡¹ç›®æˆæœ¬å½’é›†"])
            ws.append(["é¡¹ç›®ä»£ç ", "é¡¹ç›®åç§°", "æ€»æˆæœ¬", "è®°å½•æ•°"])
            for item in results['project_costs']:
                ws.append([
                    item['project_code'],
                    item['project_name'],
                    item['total_cost'],
                    item['record_count']
                ])
            ws.append([])
        
        # å†™å…¥éƒ¨é—¨æˆæœ¬
        if 'department_costs' in results and results['department_costs']:
            ws.append(["éƒ¨é—¨æˆæœ¬æ±‡æ€»"])
            ws.append(["éƒ¨é—¨", "æ€»æˆæœ¬", "æœºç¥¨", "é…’åº—", "ç«è½¦ç¥¨"])
            for item in results['department_costs']:
                ws.append([
                    item['department'],
                    item['total_cost'],
                    item['flight_cost'],
                    item['hotel_cost'],
                    item['train_cost']
                ])
            ws.append([])
        
        # å†™å…¥å¼‚å¸¸è®°å½•
        if 'anomalies' in results and results['anomalies']:
            ws.append(["äº¤å‰éªŒè¯å¼‚å¸¸"])
            ws.append(["å§“å", "æ—¥æœŸ", "å¼‚å¸¸ç±»å‹", "è€ƒå‹¤çŠ¶æ€", "è¯´æ˜"])
            for item in results['anomalies']:
                ws.append([
                    item['name'],
                    item['date'],
                    item['anomaly_type'],
                    item['attendance_status'],
                    item['description']
                ])
        
        # ä¿å­˜æ–‡ä»¶
        self.workbook.save(output_path)

        return output_path

    def get_all_project_details(self) -> List[Dict[str, Any]]:
        """
        è·å–æ‰€æœ‰é¡¹ç›®çš„è¯¦ç»†ä¿¡æ¯ï¼ˆåŒ…æ‹¬äººå‘˜ã€æ—¥æœŸèŒƒå›´ã€è¶…æ ‡ç­‰ï¼‰

        Returns:
            åŒ…å«æ‰€æœ‰é¡¹ç›®è¯¦ç»†ä¿¡æ¯çš„åˆ—è¡¨
        """
        self.logger.info("=" * 80)
        self.logger.info("å¼€å§‹è·å–æ‰€æœ‰é¡¹ç›®è¯¦ç»†ä¿¡æ¯")
        self.logger.info("=" * 80)

        results = []
        travel_sheets = ['æœºç¥¨', 'é…’åº—', 'ç«è½¦ç¥¨']
        all_records = []

        # æ”¶é›†æ‰€æœ‰å·®æ—…è®°å½•
        for sheet_name in travel_sheets:
            df = self.clean_travel_data(sheet_name)
            if df.empty or 'é¡¹ç›®' not in df.columns:
                continue

            amount_col = 'æˆä¿¡é‡‘é¢' if 'æˆä¿¡é‡‘é¢' in df.columns else 'é‡‘é¢'
            date_cols = ['å‡ºå‘æ—¥æœŸ', 'å‡ºå‘æ—¥æœŸ.1', 'å‡ºå‘æ—¶é—´', 'èµ·é£æ—¥æœŸ', 'èµ·é£æ—¥æœŸ.1', 'èµ·é£æ—¶é—´', 'èµ·é£æ—¶é—´.1', 'å…¥ä½æ—¥æœŸ', 'å…¥ä½æ—¶é—´']
            date_col = next((col for col in date_cols if col in df.columns), None)

            # è·å–è€ƒå‹¤æ•°æ®ç”¨äºéƒ¨é—¨ä¿¡æ¯ï¼ˆä½œä¸ºå¤‡ç”¨ï¼‰
            attendance_df = self.clean_attendance_data()
            person_dept_map = {}
            if not attendance_df.empty and 'å§“å' in attendance_df.columns and 'ä¸€çº§éƒ¨é—¨' in attendance_df.columns:
                person_dept_map = attendance_df[['å§“å', 'ä¸€çº§éƒ¨é—¨']].drop_duplicates().set_index('å§“å')['ä¸€çº§éƒ¨é—¨'].to_dict()

            for idx, row in df.iterrows():
                project_str = row.get('é¡¹ç›®', '')
                project_code, project_name = self.extract_project_code(project_str)
                amount = row.get(amount_col, 0)
                person = row.get('å§“å', '')
                date_val = row.get(date_col, '')

                # ä¼˜å…ˆä½¿ç”¨å·®æ—…è¡¨ä¸­çš„éƒ¨é—¨ä¿¡æ¯ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä»è€ƒå‹¤è¡¨ä¸­æŸ¥æ‰¾
                department = None
                if 'ä¸€çº§éƒ¨é—¨' in df.columns:
                    department = row.get('ä¸€çº§éƒ¨é—¨')
                    # å¤„ç†ç©ºå€¼æˆ–NaN
                    if pd.isna(department) or (isinstance(department, str) and department.strip() == ''):
                        department = None
                if not department:
                    department = person_dept_map.get(person, 'æœªçŸ¥éƒ¨é—¨')
                else:
                    department = str(department).strip()

                # å¤„ç†æ—¥æœŸ
                if pd.notna(date_val):
                    if hasattr(date_val, 'strftime'):
                        date_str = date_val.strftime('%Y-%m-%d')
                    else:
                        date_str = str(date_val)
                else:
                    date_str = ''

                # æ£€æŸ¥æ˜¯å¦è¶…æ ‡ï¼ˆéœ€è¦æ­£ç¡®åˆ¤æ–­å­—ç¬¦ä¸²"æ˜¯"æˆ–"å¦"ï¼‰
                is_over_standard = False
                over_type = ''
                over_standard_val = row.get('æ˜¯å¦è¶…æ ‡', '')
                if pd.notna(over_standard_val):
                    is_over_standard = str(over_standard_val).strip() == 'æ˜¯'
                    if is_over_standard and 'è¶…æ ‡ç±»å‹' in df.columns:
                        over_type = row.get('è¶…æ ‡ç±»å‹', '')

                # è®¡ç®—æå‰é¢„è®¢å¤©æ•°
                advance_days = None
                if 'é¢„è®¢æ—¥æœŸ' in df.columns and 'å‡ºå‘æ—¥æœŸ' in df.columns:
                    book_date = row.get('é¢„è®¢æ—¥æœŸ')
                    dep_date = row.get('å‡ºå‘æ—¥æœŸ')
                    if pd.notna(book_date) and pd.notna(dep_date):
                        try:
                            if hasattr(book_date, 'to_pydatetime'):
                                book_date = book_date.to_pydatetime()
                            if hasattr(dep_date, 'to_pydatetime'):
                                dep_date = dep_date.to_pydatetime()
                            advance_days = (dep_date - book_date).days
                        except:
                            pass

                # ç©ºé¡¹ç›®å¤„ç†
                if not project_code:
                    project_code = 'ç©ºé¡¹ç›®'
                    project_name = 'æœªåˆ†é…é¡¹ç›®'

                all_records.append({
                    'project_code': project_code,
                    'project_name': project_name,
                    'person': person,
                    'department': department,
                    'type': sheet_name,
                    'amount': amount,
                    'date': date_str,
                    'is_over_standard': bool(is_over_standard),
                    'over_type': over_type,
                    'advance_days': advance_days
                })

        if not all_records:
            self.logger.warning("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•å·®æ—…è®°å½•")
            return []

        # è½¬æ¢ä¸º DataFrame
        df_all = pd.DataFrame(all_records)

        # æŒ‰é¡¹ç›®åˆ†ç»„ç»Ÿè®¡
        grouped = df_all.groupby(['project_code', 'project_name']).agg({
            'amount': 'sum',
            'person': lambda x: list(set(x)),  # å»é‡çš„äººå‘˜åˆ—è¡¨
            'department': lambda x: list(set(x)),  # å»é‡çš„éƒ¨é—¨åˆ—è¡¨
            'date': ['min', 'max'],  # æœ€æ—©å’Œæœ€æ™šæ—¥æœŸ
            'type': 'count',  # æ€»è®¢å•æ•°
            'is_over_standard': 'sum'  # è¶…æ ‡è®¢å•æ•°
        }).reset_index()

        # å±•å¹³åˆ—å
        grouped.columns = ['project_code', 'project_name', 'total_cost', 'person_list',
                          'department_list', 'date_start', 'date_end', 'record_count', 'over_standard_count']

        # è®¡ç®—å„ç±»å‹æˆæœ¬å’Œè®¢å•æ•°
        for sheet_name in travel_sheets:
            type_df = df_all[df_all['type'] == sheet_name]
            type_grouped = type_df.groupby(['project_code', 'project_name']).agg({
                'amount': 'sum',
                'type': 'count'
            }).reset_index()
            type_grouped.columns = ['project_code', 'project_name', f'{sheet_name}_cost', f'{sheet_name}_count']
            grouped = grouped.merge(type_grouped, on=['project_code', 'project_name'], how='left')

        # å¡«å……ç©ºå€¼
        for sheet_name in travel_sheets:
            grouped[f'{sheet_name}_cost'] = grouped[f'{sheet_name}_cost'].fillna(0)
            grouped[f'{sheet_name}_count'] = grouped[f'{sheet_name}_count'].fillna(0)

        # æŒ‰æˆæœ¬é™åºæ’åº
        grouped = grouped.sort_values('total_cost', ascending=False).reset_index(drop=True)

        # æ„å»ºç»“æœ
        for _, row in grouped.iterrows():
            person_list = row['person_list'] if isinstance(row['person_list'], list) else []
            department_list = row['department_list'] if isinstance(row['department_list'], list) else []

            # æ ¼å¼åŒ–æ—¥æœŸ
            date_start = row['date_start'] if pd.notna(row['date_start']) else ''
            date_end = row['date_end'] if pd.notna(row['date_end']) else ''

            results.append({
                'code': row['project_code'],
                'name': row['project_name'],
                'total_cost': float(row['total_cost']),
                'flight_cost': float(row.get('æœºç¥¨_cost', 0)),
                'hotel_cost': float(row.get('é…’åº—_cost', 0)),
                'train_cost': float(row.get('ç«è½¦ç¥¨_cost', 0)),
                'record_count': int(row['record_count']),
                'flight_count': int(row.get('æœºç¥¨_count', 0)),
                'hotel_count': int(row.get('é…’åº—_count', 0)),
                'train_count': int(row.get('ç«è½¦ç¥¨_count', 0)),
                'person_count': len(person_list),
                'person_list': person_list,
                'department_list': department_list,
                'date_range': {
                    'start': str(date_start),
                    'end': str(date_end)
                },
                'over_standard_count': int(row['over_standard_count'])
            })

        self.logger.info(f"âœ… å…±è·å– {len(results)} ä¸ªé¡¹ç›®çš„è¯¦ç»†ä¿¡æ¯")
        self.logger.info("=" * 80 + "\n")

        return results

    def get_project_order_records(self, project_code: str) -> List[Dict[str, Any]]:
        """
        è·å–æŒ‡å®šé¡¹ç›®çš„æ‰€æœ‰è®¢å•è®°å½•

        Args:
            project_code: é¡¹ç›®ä»£ç 

        Returns:
            è¯¥é¡¹ç›®çš„æ‰€æœ‰è®¢å•è®°å½•åˆ—è¡¨
        """
        travel_sheets = ['æœºç¥¨', 'é…’åº—', 'ç«è½¦ç¥¨']
        records = []

        # è·å–è€ƒå‹¤æ•°æ®ç”¨äºéƒ¨é—¨ä¿¡æ¯
        attendance_df = self.clean_attendance_data()
        person_dept_map = {}
        if not attendance_df.empty and 'å§“å' in attendance_df.columns and 'ä¸€çº§éƒ¨é—¨' in attendance_df.columns:
            person_dept_map = attendance_df[['å§“å', 'ä¸€çº§éƒ¨é—¨']].drop_duplicates().set_index('å§“å')['ä¸€çº§éƒ¨é—¨'].to_dict()

        for sheet_name in travel_sheets:
            df = self.clean_travel_data(sheet_name)
            if df.empty or 'é¡¹ç›®' not in df.columns:
                continue

            amount_col = 'æˆä¿¡é‡‘é¢' if 'æˆä¿¡é‡‘é¢' in df.columns else 'é‡‘é¢'
            date_cols = ['å‡ºå‘æ—¥æœŸ', 'å‡ºå‘æ—¥æœŸ.1', 'å‡ºå‘æ—¶é—´', 'èµ·é£æ—¥æœŸ', 'èµ·é£æ—¥æœŸ.1', 'èµ·é£æ—¶é—´', 'èµ·é£æ—¶é—´.1', 'å…¥ä½æ—¥æœŸ', 'å…¥ä½æ—¶é—´']
            date_col = next((col for col in date_cols if col in df.columns), None)

            for idx, row in df.iterrows():
                project_str = row.get('é¡¹ç›®', '')
                extracted_code, extracted_name = self.extract_project_code(project_str)

                # ç©ºé¡¹ç›®å¤„ç†
                if not extracted_code:
                    extracted_code = 'ç©ºé¡¹ç›®'
                    extracted_name = 'æœªåˆ†é…é¡¹ç›®'

                # åŒ¹é…é¡¹ç›®ä»£ç 
                if extracted_code != project_code:
                    continue

                amount = row.get(amount_col, 0)
                person = row.get('å§“å', '')
                date_val = row.get(date_col, '')

                # ä¼˜å…ˆä½¿ç”¨å·®æ—…è¡¨ä¸­çš„éƒ¨é—¨ä¿¡æ¯ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä»è€ƒå‹¤è¡¨ä¸­æŸ¥æ‰¾
                department = None
                if 'ä¸€çº§éƒ¨é—¨' in df.columns:
                    department = row.get('ä¸€çº§éƒ¨é—¨')
                    # å¤„ç†ç©ºå€¼æˆ–NaN
                    if pd.isna(department) or (isinstance(department, str) and department.strip() == ''):
                        department = None
                if not department:
                    department = person_dept_map.get(person, 'æœªçŸ¥éƒ¨é—¨')
                else:
                    department = str(department).strip()

                # å¤„ç†æ—¥æœŸ
                if pd.notna(date_val):
                    if hasattr(date_val, 'strftime'):
                        date_str = date_val.strftime('%Y-%m-%d')
                    else:
                        date_str = str(date_val)
                else:
                    date_str = ''

                # æ£€æŸ¥æ˜¯å¦è¶…æ ‡ï¼ˆéœ€è¦æ­£ç¡®åˆ¤æ–­å­—ç¬¦ä¸²"æ˜¯"æˆ–"å¦"ï¼‰
                is_over_standard = False
                over_type = ''
                over_standard_val = row.get('æ˜¯å¦è¶…æ ‡', '')
                if pd.notna(over_standard_val):
                    is_over_standard = str(over_standard_val).strip() == 'æ˜¯'
                    if is_over_standard and 'è¶…æ ‡ç±»å‹' in df.columns:
                        over_type = row.get('è¶…æ ‡ç±»å‹', '')

                # è®¡ç®—æå‰é¢„è®¢å¤©æ•°
                advance_days = None
                if 'é¢„è®¢æ—¥æœŸ' in df.columns and 'å‡ºå‘æ—¥æœŸ' in df.columns:
                    book_date = row.get('é¢„è®¢æ—¥æœŸ')
                    dep_date = row.get('å‡ºå‘æ—¥æœŸ')
                    if pd.notna(book_date) and pd.notna(dep_date):
                        try:
                            if hasattr(book_date, 'to_pydatetime'):
                                book_date = book_date.to_pydatetime()
                            if hasattr(dep_date, 'to_pydatetime'):
                                dep_date = dep_date.to_pydatetime()
                            advance_days = (dep_date - book_date).days
                        except:
                            pass

                # è½¬æ¢ç±»å‹åç§°
                type_mapping = {'æœºç¥¨': 'flight', 'é…’åº—': 'hotel', 'ç«è½¦ç¥¨': 'train'}

                records.append({
                    'id': f"{sheet_name}_{idx}",
                    'project_code': extracted_code,
                    'project_name': extracted_name,
                    'person': person,
                    'department': department,
                    'type': type_mapping.get(sheet_name, 'other'),
                    'amount': float(amount),
                    'date': date_str,
                    'is_over_standard': bool(is_over_standard),
                    'over_type': over_type,
                    'advance_days': advance_days
                })

        # æŒ‰æ—¥æœŸæ’åº
        records.sort(key=lambda x: x['date'], reverse=True)

        return records

    def get_department_hierarchy(self) -> Dict[str, Any]:
        """
        è·å–éƒ¨é—¨å±‚çº§ç»“æ„

        Returns:
            {
                'level1': ['ä¸€çº§éƒ¨é—¨1', 'ä¸€çº§éƒ¨é—¨2', ...],
                'level2': {'ä¸€çº§éƒ¨é—¨1': ['äºŒçº§éƒ¨é—¨1', 'äºŒçº§éƒ¨é—¨2'], ...},
                'level3': {'äºŒçº§éƒ¨é—¨1': ['ä¸‰çº§éƒ¨é—¨1', 'ä¸‰çº§éƒ¨é—¨2'], ...}
            }
        """
        df = self.clean_attendance_data()
        if df.empty:
            return {'level1': [], 'level2': {}, 'level3': {}}

        result = {
            'level1': [],
            'level2': {},
            'level3': {}
        }

        # è·å–ä¸€çº§éƒ¨é—¨
        if 'ä¸€çº§éƒ¨é—¨' in df.columns:
            result['level1'] = sorted(df['ä¸€çº§éƒ¨é—¨'].dropna().unique().tolist())

        # è·å–äºŒçº§éƒ¨é—¨ï¼ˆæŒ‰ä¸€çº§éƒ¨é—¨åˆ†ç»„ï¼‰
        if 'ä¸€çº§éƒ¨é—¨' in df.columns and 'äºŒçº§éƒ¨é—¨' in df.columns:
            for l1 in result['level1']:
                l2_list = df[df['ä¸€çº§éƒ¨é—¨'] == l1]['äºŒçº§éƒ¨é—¨'].dropna().unique().tolist()
                result['level2'][l1] = sorted(l2_list)

        # è·å–ä¸‰çº§éƒ¨é—¨ï¼ˆæŒ‰äºŒçº§éƒ¨é—¨åˆ†ç»„ï¼‰
        if 'äºŒçº§éƒ¨é—¨' in df.columns and 'ä¸‰çº§éƒ¨é—¨' in df.columns:
            for l1, l2_list in result['level2'].items():
                for l2 in l2_list:
                    l3_list = df[df['äºŒçº§éƒ¨é—¨'] == l2]['ä¸‰çº§éƒ¨é—¨'].dropna().unique().tolist()
                    result['level3'][l2] = sorted(l3_list)

        return result

    def get_department_list(self, level: int, parent: Optional[str] = None) -> List[Dict[str, Any]]:
        """
        è·å–éƒ¨é—¨åˆ—è¡¨

        Args:
            level: éƒ¨é—¨å±‚çº§ (1=ä¸€çº§, 2=äºŒçº§, 3=ä¸‰çº§)
            parent: çˆ¶éƒ¨é—¨åç§°ï¼ˆlevel>1æ—¶å¿…éœ€ï¼‰

        Returns:
            éƒ¨é—¨åˆ—è¡¨ï¼Œæ¯ä¸ªéƒ¨é—¨åŒ…å«äººæ•°ã€æˆæœ¬ã€å¹³å‡å·¥æ—¶ç­‰ä¿¡æ¯
        """
        df = self.clean_attendance_data()
        if df.empty:
            self.logger.warning(f"get_department_list: è€ƒå‹¤æ•°æ®ä¸ºç©ºï¼Œlevel={level}, parent={parent}")
            return []

        # ç¡®å®šéƒ¨é—¨åˆ—åï¼ˆä¸­æ–‡æ•°å­—ï¼šä¸€çº§éƒ¨é—¨ã€äºŒçº§éƒ¨é—¨ã€ä¸‰çº§éƒ¨é—¨ï¼‰
        level_name_map = {1: 'ä¸€çº§éƒ¨é—¨', 2: 'äºŒçº§éƒ¨é—¨', 3: 'ä¸‰çº§éƒ¨é—¨'}
        dept_col = level_name_map.get(level)

        if not dept_col or dept_col not in df.columns:
            self.logger.warning(f"get_department_list: ç¼ºå°‘éƒ¨é—¨åˆ— '{dept_col}'ï¼ˆlevel={level}ï¼‰ï¼Œå¯ç”¨åˆ—: {df.columns.tolist()}")
            return []

        # ç­›é€‰éƒ¨é—¨
        filtered_df = df.copy()

        if level == 2 and parent:
            filtered_df = filtered_df[filtered_df['ä¸€çº§éƒ¨é—¨'] == parent]
        elif level == 3 and parent:
            filtered_df = filtered_df[filtered_df['äºŒçº§éƒ¨é—¨'] == parent]

        # æ£€æŸ¥ç­›é€‰åçš„æ•°æ®
        if filtered_df.empty:
            self.logger.warning(f"get_department_list: ç­›é€‰åæ•°æ®ä¸ºç©ºï¼Œlevel={level}, parent={parent}")
            return []

        departments = filtered_df[dept_col].dropna().unique().tolist()
        self.logger.info(f"get_department_list: æ‰¾åˆ° {len(departments)} ä¸ª{dept_col}: {departments[:5]}...")

        # è·å–å·®æ—…æ•°æ®ç”¨äºæˆæœ¬è®¡ç®—
        dept_costs = self._calculate_costs_by_department(filtered_df, dept_col)

        results = []
        for dept in departments:
            dept_data = filtered_df[filtered_df[dept_col] == dept]

            # è®¡ç®—äººæ•°
            person_count = dept_data['å§“å'].nunique() if 'å§“å' in dept_data.columns else 0

            # è®¡ç®—å¹³å‡å·¥æ—¶
            avg_hours = 0
            if 'å·¥æ—¶' in dept_data.columns:
                valid_hours = dept_data[dept_data['å·¥æ—¶'] != 0]['å·¥æ—¶'].dropna()
                if not valid_hours.empty:
                    avg_hours = float(valid_hours.mean())

            # è·å–æˆæœ¬
            cost_info = dept_costs.get(dept, {'total_cost': 0, 'flight_cost': 0, 'hotel_cost': 0, 'train_cost': 0})

            results.append({
                'name': dept,
                'level': level,
                'parent': parent,
                'person_count': int(person_count),
                'total_cost': float(cost_info['total_cost']),
                'avg_work_hours': round(avg_hours, 2)
            })

        # æŒ‰æˆæœ¬é™åºæ’åº
        results.sort(key=lambda x: x['total_cost'], reverse=True)

        self.logger.info(f"get_department_list: è¿”å› {len(results)} ä¸ªéƒ¨é—¨ï¼Œå‰3ä¸ª: {[(r['name'], r['total_cost'], r['person_count']) for r in results[:3]]}")

        return results

    def _calculate_costs_by_department(self, attendance_df: pd.DataFrame, dept_col: str) -> Dict[str, Dict[str, float]]:
        """
        è®¡ç®—å„éƒ¨é—¨çš„å·®æ—…æˆæœ¬

        Args:
            attendance_df: è€ƒå‹¤æ•°æ®ï¼ˆå·²æŒ‰éƒ¨é—¨ç­›é€‰ï¼‰
            dept_col: éƒ¨é—¨åˆ—å

        Returns:
            {éƒ¨é—¨å: {total_cost, flight_cost, hotel_cost, train_cost}}
        """
        dept_costs = {}

        if attendance_df.empty:
            self.logger.warning(f"_calculate_costs_by_department: attendance_df ä¸ºç©º")
            return dept_costs

        # è·å–å§“ååˆ°éƒ¨é—¨çš„æ˜ å°„ï¼ˆä¿ç•™æ‰€æœ‰æ˜ å°„ï¼Œä¸€ä¸ªäººå¯èƒ½å±äºå¤šä¸ªéƒ¨é—¨ï¼‰
        name_dept_map = {}
        for _, row in attendance_df[['å§“å', dept_col]].drop_duplicates().iterrows():
            name = row.get('å§“å')
            dept = row.get(dept_col)
            if not name or pd.isna(name) or not dept or pd.isna(dept):
                continue
            if name not in name_dept_map:
                name_dept_map[name] = []
            if dept not in name_dept_map[name]:
                name_dept_map[name].append(dept)

        # éå†å·®æ—…æ•°æ®
        travel_sheets = ['æœºç¥¨', 'é…’åº—', 'ç«è½¦ç¥¨']
        cost_keys = {'æœºç¥¨': 'flight_cost', 'é…’åº—': 'hotel_cost', 'ç«è½¦ç¥¨': 'train_cost'}

        total_records = 0
        matched_records = 0

        for sheet_name in travel_sheets:
            df = self.clean_travel_data(sheet_name)
            if df.empty:
                continue

            amount_col = 'æˆä¿¡é‡‘é¢' if 'æˆä¿¡é‡‘é¢' in df.columns else 'é‡‘é¢'

            for _, row in df.iterrows():
                total_records += 1
                name = row.get('å§“å', '')
                if not name or pd.isna(name):
                    continue

                depts = name_dept_map.get(name, [])
                if not depts:
                    continue

                matched_records += 1
                amount = row.get(amount_col, 0) or 0
                cost_key = cost_keys[sheet_name]

                # ä¸€ä¸ªäººå¯èƒ½å±äºå¤šä¸ªéƒ¨é—¨ï¼Œå°†æˆæœ¬åˆ†é…åˆ°æ‰€æœ‰å…³è”éƒ¨é—¨
                for dept in depts:
                    if dept not in dept_costs:
                        dept_costs[dept] = {'total_cost': 0, 'flight_cost': 0, 'hotel_cost': 0, 'train_cost': 0}
                    dept_costs[dept][cost_key] += amount
                    dept_costs[dept]['total_cost'] += amount

        self.logger.info(f"_calculate_costs_by_department: å·®æ—…è®°å½• {total_records} æ¡ï¼ŒåŒ¹é… {matched_records} æ¡ï¼Œéƒ¨é—¨æ•° {len(dept_costs)}")
        return dept_costs

    def get_department_detail_metrics(self, department_name: str, level: int = 3) -> Dict[str, Any]:
        """
        è·å–æŒ‡å®šéƒ¨é—¨çš„è¯¦ç»†æŒ‡æ ‡ï¼ˆ12é¡¹ï¼‰

        Args:
            department_name: éƒ¨é—¨åç§°
            level: éƒ¨é—¨å±‚çº§ (1=ä¸€çº§, 2=äºŒçº§, 3=ä¸‰çº§)

        Returns:
            åŒ…å«12é¡¹æŒ‡æ ‡çš„å­—å…¸
        """
        df = self.clean_attendance_data()
        if df.empty:
            return {}

        # ç¡®å®šéƒ¨é—¨åˆ—åï¼ˆä½¿ç”¨ä¸­æ–‡æ•°å­—ï¼šä¸€çº§éƒ¨é—¨ã€äºŒçº§éƒ¨é—¨ã€ä¸‰çº§éƒ¨é—¨ï¼‰
        level_name_map = {1: 'ä¸€çº§éƒ¨é—¨', 2: 'äºŒçº§éƒ¨é—¨', 3: 'ä¸‰çº§éƒ¨é—¨'}
        dept_col = level_name_map.get(level)
        if not dept_col or dept_col not in df.columns:
            return {}

        # ç­›é€‰è¯¥éƒ¨é—¨çš„æ•°æ®
        dept_df = df[df[dept_col] == department_name].copy()

        if dept_df.empty:
            return {}

        # è·å–çˆ¶éƒ¨é—¨
        parent_dept = None
        if level == 2 and 'ä¸€çº§éƒ¨é—¨' in dept_df.columns:
            parent_dept = dept_df['ä¸€çº§éƒ¨é—¨'].dropna().unique()
            parent_dept = parent_dept[0] if len(parent_dept) > 0 else None
        elif level == 3 and 'äºŒçº§éƒ¨é—¨' in dept_df.columns:
            parent_dept = dept_df['äºŒçº§éƒ¨é—¨'].dropna().unique()
            parent_dept = parent_dept[0] if len(parent_dept) > 0 else None

        # 1. å½“æœˆè€ƒå‹¤å¤©æ•°åˆ†å¸ƒ
        attendance_days_distribution = {}
        if 'å½“æ—¥çŠ¶æ€åˆ¤æ–­' in dept_df.columns:
            attendance_days_distribution = dept_df['å½“æ—¥çŠ¶æ€åˆ¤æ–­'].value_counts().to_dict()

        # 2. å…¬ä¼‘æ—¥ä¸Šç­å¤©æ•°
        weekend_work_days = 0
        if 'å½“æ—¥çŠ¶æ€åˆ¤æ–­' in dept_df.columns:
            weekend_work_days = int(dept_df[dept_df['å½“æ—¥çŠ¶æ€åˆ¤æ–­'] == 'å…¬ä¼‘æ—¥ä¸Šç­'].shape[0])

        # 3. å·¥ä½œæ—¥å‡ºå‹¤å¤©æ•°
        workday_attendance_days = 0
        if 'å½“æ—¥çŠ¶æ€åˆ¤æ–­' in dept_df.columns:
            workday_attendance_days = int(dept_df[dept_df['å½“æ—¥çŠ¶æ€åˆ¤æ–­'] == 'ä¸Šç­'].shape[0])

        # 4. å·¥ä½œæ—¥å¹³å‡å·¥æ—¶
        avg_work_hours = 0
        if 'å·¥æ—¶' in dept_df.columns:
            valid_hours = dept_df[(dept_df['å½“æ—¥çŠ¶æ€åˆ¤æ–­'] == 'ä¸Šç­') & (dept_df['å·¥æ—¶'] != 0)]['å·¥æ—¶'].dropna()
            if not valid_hours.empty:
                avg_work_hours = float(valid_hours.mean())

        # 5. å‡ºå·®å¤©æ•°
        travel_days = 0
        if 'å½“æ—¥çŠ¶æ€åˆ¤æ–­' in dept_df.columns:
            travel_days = int(dept_df[dept_df['å½“æ—¥çŠ¶æ€åˆ¤æ–­'] == 'å‡ºå·®'].shape[0])

        # 6. è¯·å‡å¤©æ•°
        leave_days = 0
        if 'å½“æ—¥çŠ¶æ€åˆ¤æ–­' in dept_df.columns:
            leave_days = int(dept_df[dept_df['å½“æ—¥çŠ¶æ€åˆ¤æ–­'] == 'è¯·å‡'].shape[0])

        # 7. å¼‚å¸¸å¤©æ•°ï¼ˆé€šè¿‡äº¤å‰éªŒè¯ï¼‰
        anomalies = self.cross_check_attendance_travel()
        dept_anomalies = [a for a in anomalies if a.get('department') == department_name]
        anomaly_days = len(dept_anomalies)

        # 8. æ™šä¸Š7:30åä¸‹ç­äººæ•°
        late_after_1930_count = 0
        if 'æœ€æ™š19:30ä¹‹å' in dept_df.columns:
            late_after_1930_count = int(dept_df[dept_df['æœ€æ™š19:30ä¹‹å'] == 'ç¬¦åˆ']['å§“å'].nunique())

        # 9. å‘¨æœ«å‡ºå‹¤æ¬¡æ•°
        weekend_attendance_count = 0
        if 'æ—¥æœŸ' in dept_df.columns and 'å½“æ—¥çŠ¶æ€åˆ¤æ–­' in dept_df.columns:
            dept_df['weekday'] = dept_df['æ—¥æœŸ'].dt.dayofweek
            weekend_df = dept_df[dept_df['weekday'].isin([5, 6])]  # 5=å‘¨å…­, 6=å‘¨æ—¥
            weekend_attendance_count = int(weekend_df[weekend_df['å½“æ—¥çŠ¶æ€åˆ¤æ–­'].isin(['ä¸Šç­', 'å‡ºå·®'])].shape[0])

        # 10. å‡ºå·®æ’è¡Œæ¦œï¼ˆæŒ‰å‡ºå·®å¤©æ•°ï¼‰
        travel_ranking = []
        if 'å½“æ—¥çŠ¶æ€åˆ¤æ–­' in dept_df.columns:
            travel_df = dept_df[dept_df['å½“æ—¥çŠ¶æ€åˆ¤æ–­'] == 'å‡ºå·®']
            if not travel_df.empty and 'å§“å' in travel_df.columns:
                travel_counts = travel_df['å§“å'].value_counts().head(10)
                travel_ranking = [
                    {'name': name, 'value': int(count), 'detail': f'{count}å¤©'}
                    for name, count in travel_counts.items()
                ]

        # 11. å¼‚å¸¸æ’è¡Œæ¦œï¼ˆæŒ‰å¼‚å¸¸æ¬¡æ•°ï¼‰
        anomaly_ranking = []
        if dept_anomalies:
            from collections import Counter
            anomaly_counts = Counter([a.get('name', '') for a in dept_anomalies])
            anomaly_ranking = [
                {'name': name, 'value': int(count), 'detail': f'{count}æ¬¡'}
                for name, count in anomaly_counts.most_common(10)
            ]

        # 12. æœ€æ™šä¸‹ç­æ’è¡Œæ¦œ
        latest_checkout_ranking = []
        if 'æœ€æ™šæ‰“å¡æ—¶é—´' in dept_df.columns:
            dept_df['punch_time'] = pd.to_datetime(dept_df['æœ€æ™šæ‰“å¡æ—¶é—´'], format='%H:%M:%S', errors='coerce')
            valid_punch = dept_df[dept_df['punch_time'].notna()].sort_values('punch_time', ascending=False)
            if not valid_punch.empty and 'å§“å' in valid_punch.columns:
                for _, row in valid_punch.head(10).iterrows():
                    latest_checkout_ranking.append({
                        'name': row['å§“å'],
                        'value': 0,  # EChartséœ€è¦æ•°å€¼ï¼Œè¿™é‡Œä»…ç”¨äºæ’åº
                        'detail': row['æœ€æ™šæ‰“å¡æ—¶é—´']
                    })

        # 13. æœ€é•¿å·¥æ—¶æ’è¡Œæ¦œï¼ˆæŒ‰å¹³å‡å·¥æ—¶æ’åï¼‰
        longest_hours_ranking = []
        if 'å·¥æ—¶' in dept_df.columns and 'å§“å' in dept_df.columns:
            # å…ˆæŒ‰äººå‘˜åˆ†ç»„è®¡ç®—å¹³å‡å·¥æ—¶ï¼Œæ’é™¤å·¥æ—¶ä¸º0çš„è®°å½•
            person_avg_hours = dept_df[dept_df['å·¥æ—¶'].notna() & (dept_df['å·¥æ—¶'] != 0)].groupby('å§“å')['å·¥æ—¶'].mean()
            # æŒ‰å¹³å‡å·¥æ—¶é™åºæ’åˆ—
            person_avg_hours = person_avg_hours.sort_values(ascending=False)
            # å–å‰10å
            for name, avg_hours in person_avg_hours.head(10).items():
                longest_hours_ranking.append({
                    'name': name,
                    'value': float(round(avg_hours, 2)),
                    'detail': f'{avg_hours:.2f}å°æ—¶'
                })

        return {
            'department_name': department_name,
            'department_level': f'{level}çº§éƒ¨é—¨',
            'parent_department': parent_dept,
            'attendance_days_distribution': attendance_days_distribution,
            'weekend_work_days': weekend_work_days,
            'workday_attendance_days': workday_attendance_days,
            'avg_work_hours': round(avg_work_hours, 2),
            'travel_days': travel_days,
            'leave_days': leave_days,
            'anomaly_days': anomaly_days,
            'late_after_1930_count': late_after_1930_count,
            'weekend_attendance_count': weekend_attendance_count,
            'travel_ranking': travel_ranking,
            'anomaly_ranking': anomaly_ranking,
            'latest_checkout_ranking': latest_checkout_ranking,
            'longest_hours_ranking': longest_hours_ranking
        }

    def get_level1_department_statistics(self, level1_name: str) -> Dict[str, Any]:
        """
        è·å–ä¸€çº§éƒ¨é—¨çš„æ±‡æ€»ç»Ÿè®¡æ•°æ®ï¼ˆç”¨äºäºŒçº§éƒ¨é—¨è¡¨æ ¼ä¸‹æ–¹çš„ç»Ÿè®¡å±•ç¤ºï¼‰

        Args:
            level1_name: ä¸€çº§éƒ¨é—¨åç§°

        Returns:
            åŒ…å«ä»¥ä¸‹ç»Ÿè®¡æ•°æ®çš„å­—å…¸:
            - total_travel_cost: ç´¯è®¡å·®æ—…æˆæœ¬
            - attendance_days_distribution: è€ƒå‹¤å¤©æ•°åˆ†å¸ƒ
            - travel_ranking: å‡ºå·®æ’è¡Œæ¦œï¼ˆæŒ‰äººï¼‰
            - avg_hours_ranking: å¹³å‡å·¥æ—¶æ’è¡Œæ¦œï¼ˆæŒ‰äººï¼‰
            - level2_department_stats: äºŒçº§éƒ¨é—¨ç»Ÿè®¡åˆ—è¡¨ï¼ˆåŒ…å«æ‰€æœ‰æŒ‡æ ‡ï¼‰
        """
        df = self.clean_attendance_data()
        if df.empty:
            return {}

        # ç­›é€‰è¯¥ä¸€çº§éƒ¨é—¨çš„æ•°æ®
        level1_df = df[df['ä¸€çº§éƒ¨é—¨'] == level1_name].copy()

        if level1_df.empty:
            return {}

        # 1. ç´¯è®¡å·®æ—…æˆæœ¬
        total_travel_cost = 0
        dept_costs = self._calculate_costs_by_department(level1_df, 'äºŒçº§éƒ¨é—¨')
        for cost_info in dept_costs.values():
            total_travel_cost += cost_info['total_cost']

        # 2. è€ƒå‹¤å¤©æ•°åˆ†å¸ƒï¼ˆæ•´ä¸ªä¸€çº§éƒ¨é—¨ï¼‰
        attendance_days_distribution = {}
        if 'å½“æ—¥çŠ¶æ€åˆ¤æ–­' in level1_df.columns:
            attendance_days_distribution = level1_df['å½“æ—¥çŠ¶æ€åˆ¤æ–­'].value_counts().to_dict()

        # 3. å‡ºå·®æ’è¡Œæ¦œï¼ˆæŒ‰äººï¼Œåœ¨æ•´ä¸ªä¸€çº§éƒ¨é—¨èŒƒå›´å†…ï¼‰
        travel_ranking = []
        if 'å½“æ—¥çŠ¶æ€åˆ¤æ–­' in level1_df.columns:
            travel_df = level1_df[level1_df['å½“æ—¥çŠ¶æ€åˆ¤æ–­'] == 'å‡ºå·®']
            if not travel_df.empty and 'å§“å' in travel_df.columns:
                travel_counts = travel_df['å§“å'].value_counts().head(10)
                travel_ranking = [
                    {'name': name, 'value': int(count), 'detail': f'{count}å¤©'}
                    for name, count in travel_counts.items()
                ]

        # 4. å¹³å‡å·¥æ—¶æ’è¡Œæ¦œï¼ˆæŒ‰äººï¼Œåœ¨æ•´ä¸ªä¸€çº§éƒ¨é—¨èŒƒå›´å†…ï¼‰
        avg_hours_ranking = []
        if 'å·¥æ—¶' in level1_df.columns and 'å§“å' in level1_df.columns:
            person_avg_hours = level1_df[level1_df['å·¥æ—¶'].notna() & (level1_df['å·¥æ—¶'] != 0)].groupby('å§“å')['å·¥æ—¶'].mean()
            person_avg_hours = person_avg_hours.sort_values(ascending=False)
            for name, avg_hours in person_avg_hours.head(10).items():
                avg_hours_ranking.append({
                    'name': name,
                    'value': float(round(avg_hours, 2)),
                    'detail': f'{avg_hours:.2f}å°æ—¶'
                })

        # 5. äºŒçº§éƒ¨é—¨ç»Ÿè®¡ï¼ˆåŒ…å«æ‰€æœ‰æŒ‡æ ‡ï¼‰
        level2_department_stats = []
        if 'äºŒçº§éƒ¨é—¨' in level1_df.columns:
            level2_list = level1_df['äºŒçº§éƒ¨é—¨'].dropna().unique().tolist()

            for l2_dept in level2_list:
                l2_df = level1_df[level1_df['äºŒçº§éƒ¨é—¨'] == l2_dept]

                # è®¡ç®—äººæ•°
                person_count = l2_df['å§“å'].nunique() if 'å§“å' in l2_df.columns else 0

                # è®¡ç®—å¹³å‡å·¥æ—¶
                avg_hours = 0
                if 'å·¥æ—¶' in l2_df.columns:
                    valid_hours = l2_df[(l2_df['å½“æ—¥çŠ¶æ€åˆ¤æ–­'] == 'ä¸Šç­') & (l2_df['å·¥æ—¶'] != 0)]['å·¥æ—¶'].dropna()
                    if not valid_hours.empty:
                        avg_hours = float(valid_hours.mean())

                # å·¥ä½œæ—¥å‡ºå‹¤å¤©æ•°
                workday_attendance_days = 0
                if 'å½“æ—¥çŠ¶æ€åˆ¤æ–­' in l2_df.columns:
                    workday_attendance_days = int(l2_df[l2_df['å½“æ—¥çŠ¶æ€åˆ¤æ–­'] == 'ä¸Šç­'].shape[0])

                # å…¬ä¼‘æ—¥ä¸Šç­å¤©æ•°
                weekend_work_days = 0
                if 'å½“æ—¥çŠ¶æ€åˆ¤æ–­' in l2_df.columns:
                    weekend_work_days = int(l2_df[l2_df['å½“æ—¥çŠ¶æ€åˆ¤æ–­'] == 'å…¬ä¼‘æ—¥ä¸Šç­'].shape[0])

                # å‘¨æœ«å‡ºå‹¤æ¬¡æ•°
                weekend_attendance_count = 0
                if 'æ—¥æœŸ' in l2_df.columns and 'å½“æ—¥çŠ¶æ€åˆ¤æ–­' in l2_df.columns:
                    l2_df_copy = l2_df.copy()
                    l2_df_copy['weekday'] = l2_df_copy['æ—¥æœŸ'].dt.dayofweek
                    weekend_df = l2_df_copy[l2_df_copy['weekday'].isin([5, 6])]
                    weekend_attendance_count = int(weekend_df[weekend_df['å½“æ—¥çŠ¶æ€åˆ¤æ–­'].isin(['ä¸Šç­', 'å‡ºå·®'])].shape[0])

                # å‡ºå·®å¤©æ•°
                travel_days = 0
                if 'å½“æ—¥çŠ¶æ€åˆ¤æ–­' in l2_df.columns:
                    travel_days = int(l2_df[l2_df['å½“æ—¥çŠ¶æ€åˆ¤æ–­'] == 'å‡ºå·®'].shape[0])

                # è¯·å‡å¤©æ•°
                leave_days = 0
                if 'å½“æ—¥çŠ¶æ€åˆ¤æ–­' in l2_df.columns:
                    leave_days = int(l2_df[l2_df['å½“æ—¥çŠ¶æ€åˆ¤æ–­'] == 'è¯·å‡'].shape[0])

                # å¼‚å¸¸å¤©æ•°
                anomalies = self.cross_check_attendance_travel()
                dept_anomalies = [a for a in anomalies if a.get('department') == l2_dept]
                anomaly_days = len(dept_anomalies)

                # æ™šä¸Š7:30åä¸‹ç­äººæ•°
                late_after_1930_count = 0
                if 'æœ€æ™š19:30ä¹‹å' in l2_df.columns:
                    late_after_1930_count = int(l2_df[l2_df['æœ€æ™š19:30ä¹‹å'] == 'ç¬¦åˆ']['å§“å'].nunique())

                # è·å–è¯¥äºŒçº§éƒ¨é—¨çš„æˆæœ¬
                cost_info = dept_costs.get(l2_dept, {'total_cost': 0})

                level2_department_stats.append({
                    'name': l2_dept,
                    'person_count': person_count,
                    'avg_work_hours': round(avg_hours, 2),
                    'workday_attendance_days': workday_attendance_days,
                    'weekend_work_days': weekend_work_days,
                    'weekend_attendance_count': weekend_attendance_count,
                    'travel_days': travel_days,
                    'leave_days': leave_days,
                    'anomaly_days': anomaly_days,
                    'late_after_1930_count': late_after_1930_count,
                    'total_cost': float(cost_info['total_cost'])
                })

            # æŒ‰æˆæœ¬é™åºæ’åº
            level2_department_stats.sort(key=lambda x: x['total_cost'], reverse=True)

        return {
            'department_name': level1_name,
            'total_travel_cost': round(total_travel_cost, 2),
            'attendance_days_distribution': attendance_days_distribution,
            'travel_ranking': travel_ranking,
            'avg_hours_ranking': avg_hours_ranking,
            'level2_department_stats': level2_department_stats
        }

    def get_available_months(self) -> List[str]:
        """è·å–æ‰€æœ‰å¯ç”¨çš„æœˆä»½åˆ—è¡¨ï¼ˆä»å·®æ—…æ•°æ®ä¸­æå–ï¼Œæ ¼å¼ï¼šYYYY-Mï¼ŒæŒ‰æ—¶é—´å‡åºæ’åˆ—ï¼‰"""
        # Ensure data is loaded
        if not self.sheets_data:
            self.load_all_sheets()

        months_set = set()

        flight_df = self.clean_travel_data('æœºç¥¨')
        if not flight_df.empty and 'èµ·é£æ—¥æœŸ' in flight_df.columns:
            months = flight_df['èµ·é£æ—¥æœŸ'].dt.strftime('%Y-%m').dropna().unique()
            months_set.update(months)

        hotel_df = self.clean_travel_data('é…’åº—')
        if not hotel_df.empty and 'å…¥ä½æ—¥æœŸ' in hotel_df.columns:
            months = hotel_df['å…¥ä½æ—¥æœŸ'].dt.strftime('%Y-%m').dropna().unique()
            months_set.update(months)

        train_df = self.clean_travel_data('ç«è½¦ç¥¨')
        if not train_df.empty and 'å‡ºå‘æ—¥æœŸ' in train_df.columns:
            months = train_df['å‡ºå‘æ—¥æœŸ'].dt.strftime('%Y-%m').dropna().unique()
            months_set.update(months)

        return sorted(list(months_set))

    def _save_cache(self, cache_path: str, data: Dict[str, Any]):
        """Save analysis results to JSON cache file"""
        import json
        from pathlib import Path

        cache_file = Path(cache_path)
        cache_file.parent.mkdir(parents=True, exist_ok=True)

        temp_path = cache_file.with_suffix('.tmp')
        try:
            with open(temp_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            temp_path.replace(cache_file)
            self.logger.info(f"Cache saved to: {cache_path}")
        except Exception as e:
            self.logger.error(f"Failed to save cache: {e}")
            if temp_path.exists():
                temp_path.unlink()
            raise
